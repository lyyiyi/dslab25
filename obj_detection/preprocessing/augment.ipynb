{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np \n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import shutil\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms import v2, PILToTensor\n",
    "from torchvision.transforms.functional import pil_to_tensor\n",
    "from torchvision.io import read_image, write_jpeg\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "from torchvision import tv_tensors\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = os.getcwd().split('dslab25')[0] + 'dslab25/'\n",
    "root_dir = repo_dir + \"training/vacuum_pump\"\n",
    "# root_dir = repo_dir + \"training/qwen\"\n",
    "original_images = os.path.join(root_dir, \"images/original\")\n",
    "original_annotations = os.path.join(root_dir, \"annotation/original\")\n",
    "base_dir_images = os.path.join(root_dir, \"images/augmented\")\n",
    "base_dir_annotations = os.path.join(root_dir, \"annotation/augmented\")\n",
    "out_base_images = os.path.join(root_dir, \"images/augmented\")\n",
    "out_base_annotations = os.path.join(root_dir, \"annotation/augmented\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STAGES = 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy orignals to augmented folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copytree(original_images, base_dir_images)\n",
    "shutil.copytree(original_annotations, base_dir_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete screw permutations (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def delete_non_hello_files(folder_path, perm_number_to_keep):\n",
    "# \t\"\"\"\n",
    "# \tDeletes all files in the specified folder that do not contain 'hello' in their name.\n",
    "\t\n",
    "# \tArgs:\n",
    "# \t\tfolder_path (str): Path to the target folder.\n",
    "# \t\"\"\"\n",
    "# \tif not os.path.isdir(folder_path):\n",
    "# \t\tprint(f\"The path {folder_path} is not a valid directory.\")\n",
    "# \t\treturn\n",
    "\n",
    "# \tfor filename in os.listdir(folder_path):\n",
    "# \t\tfile_path = os.path.join(folder_path, filename)\n",
    "# \t\tif os.path.isfile(file_path) and perm_number_to_keep not in filename:\n",
    "# \t\t\ttry:\n",
    "# \t\t\t\tos.remove(file_path)\n",
    "# \t\t\t\tprint(f\"Deleted: {filename}\")\n",
    "# \t\t\texcept Exception as e:\n",
    "# \t\t\t\tprint(f\"Could not delete {filename}: {e}\")\n",
    "# delete_non_hello_files(os.path.join(base_dir_images, \"stage_5\"), \"perm_7\")\n",
    "# delete_non_hello_files(os.path.join(base_dir_images, \"stage_7\"), \"perm_31\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotate (images takes up to 2 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rotate_image(image_path, out_image_path, out_annotation_path, angle):\n",
    "\ttry:\n",
    "\t\t# Try using torchvision's read_image first\n",
    "\t\ttry:\n",
    "\t\t\timage = read_image(image_path)\n",
    "\t\texcept:\n",
    "\t\t\t# Fallback to PIL\n",
    "\t\t\tpil_image = Image.open(image_path).convert(\"RGB\")\n",
    "\t\t\timage = PILToTensor()(pil_image)\n",
    "\t\t\n",
    "\t\t# Create rotation transformation with expand=True to avoid clipping\n",
    "\t\ttransform = v2.Compose([\n",
    "\t\t\tv2.RandomRotation(degrees=(angle, angle), expand=True)\n",
    "\t\t\t\n",
    "\t\t])\n",
    "\t\t\n",
    "\t\t# Apply rotation to image\n",
    "\t\trotated_image = transform(image)\n",
    "\t\t\n",
    "\t\t# Save rotated image\n",
    "\t\twrite_jpeg(rotated_image, out_image_path, quality=95)\n",
    "\t\t\n",
    "\t\t# Create empty annotation file\n",
    "\t\twith open(out_annotation_path, 'w') as f:\n",
    "\t\t\tpass\n",
    "\t\t\t\n",
    "\t\treturn True\n",
    "\t\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"Error processing {image_path}: {str(e)}\")\n",
    "\t\treturn False\n",
    "\n",
    "img_counter = 0\n",
    "stages = [f\"stage_{i}\" for i in range(N_STAGES)]\n",
    "for stage in stages:\n",
    "\timage_folder = os.path.join(base_dir_images, stage)\n",
    "\tin_image_folder = os.path.join(base_dir_images, stage)\n",
    "\tin_annotation_folder = os.path.join(base_dir_annotations, stage)\n",
    "\tout_image_folder = os.path.join(out_base_images, stage)\n",
    "\tout_annotation_folder = os.path.join(out_base_annotations, stage)\n",
    "\t\t\n",
    "\t# Create output directories if they don't exist\n",
    "\tos.makedirs(out_image_folder, exist_ok=True)\n",
    "\tos.makedirs(out_annotation_folder, exist_ok=True)\n",
    "\n",
    "\t# List only original images (skip already augmented ones)\n",
    "\timage_files = [f for f in os.listdir(image_folder) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\")) and \"_rot\" not in f]\n",
    "\n",
    "\tdef process_rotation(task):\n",
    "\t\tfilename, angle = task\n",
    "\t\timage_path = os.path.join(image_folder, filename)\n",
    "\t\tbase_filename = os.path.splitext(filename)[0]\n",
    "\t\tannotation_filename = base_filename + \".txt\"\n",
    "\t\tannotation_path = os.path.join(in_annotation_folder, annotation_filename)\n",
    "\n",
    "\t\tout_image_filename = f\"{base_filename}_rot{angle}.jpg\"\n",
    "\t\tout_annotation_filename = f\"{base_filename}_rot{angle}.txt\"\n",
    "\t\tout_image_path = os.path.join(out_image_folder, out_image_filename)\n",
    "\t\tout_annotation_path = os.path.join(out_annotation_folder, out_annotation_filename)\n",
    "\n",
    "\t\tsuccess = rotate_image(image_path, out_image_path, out_annotation_path, angle)\n",
    "\n",
    "\t\t# Copy annotation (comment this block if not using qwen)\n",
    "\t\tif os.path.exists(annotation_path):\n",
    "\t\t\twith open(annotation_path, 'r') as f:\n",
    "\t\t\t\tcontent = f.read()\n",
    "\t\t\twith open(out_annotation_path, 'w') as f:\n",
    "\t\t\t\tf.write(content)\n",
    "\n",
    "\t\treturn (out_image_path, angle, success)\n",
    "\n",
    "\t# Prepare tasks: (filename, angle)\n",
    "\ttasks = [(filename, angle) for filename in image_files for angle in range(20, 361, 20)]\n",
    "\n",
    "\t# Parallel execution\n",
    "\twith ThreadPoolExecutor(max_workers=16) as executor:\n",
    "\t\tfor idx, result in enumerate(executor.map(process_rotation, tasks)):\n",
    "\t\t\tout_image_path, angle, success = result\n",
    "\t\t\tif success:\n",
    "\t\t\t\tif idx % 1000 == 0:\n",
    "\t\t\t\t\tprint(f\"Processed {out_image_path} with rotation {angle}Â°\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotate (labels correctly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this you need to label all 18 (360/20) rotations of render image 1, 2, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the directories\n",
    "BASE_LABELS_DIR = os.path.join(os.getcwd(), 'stage_0/labels/')\n",
    "\n",
    "# Regular expressions for base and annotation files\n",
    "base_pattern = re.compile(r'^stage_0_case_render_([124])_rot(\\d+)\\.txt$')\n",
    "# For base we expect files: render_1, render_2, render_4.\n",
    "# For the annotation files, we ignore any _perm_x part:\n",
    "annot_pattern = re.compile(r'^stage_(\\d+)(?:_perm_\\d+)?(?:_var_\\d+)?_case_render_(\\d+)_rot(\\d+)\\.txt$')\n",
    "\n",
    "# Data structure to hold rotation-specific base info\n",
    "# key: rotation (as string), value: dict with base, col_shift, row_shift, w, h\n",
    "rotation_data = {}\n",
    "\n",
    "# First, process the base folder and group by rotation\n",
    "# We need to read render_1, render_2, and render_4 for each rotation\n",
    "for fname in os.listdir(BASE_LABELS_DIR):\n",
    "\tmatch = base_pattern.match(fname)\n",
    "\tif not match:\n",
    "\t\tcontinue\n",
    "\trender_number, rot = match.groups()\n",
    "\tpath = os.path.join(BASE_LABELS_DIR, fname)\n",
    "\twith open(path, 'r') as f:\n",
    "\t\t# Assume each file has one line like \"0 x y w h\"\n",
    "\t\tparts = f.read().strip().split()\n",
    "\t\t# Convert numeric values (skip the class since we'll use our own later)\n",
    "\t\t# Order: class, x, y, w, h\n",
    "\t\ttry:\n",
    "\t\t\t_, x, y, w, h = parts\n",
    "\t\t\tx, y, w, h = float(x), float(y), float(w), float(h)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f\"Error processing {fname}: {e}\")\n",
    "\t\t\tcontinue\n",
    "\n",
    "\tif rot not in rotation_data:\n",
    "\t\trotation_data[rot] = {}\n",
    "\trotation_data[rot][f'render_{render_number}'] = (x, y, w, h)\n",
    "\n",
    "# Now, compute for each rotation the base values, col_shift and row_shift\n",
    "for rot, data in rotation_data.items():\n",
    "\ttry:\n",
    "\t\tbase_x, base_y, w, h = data['render_1']\n",
    "\t\tcol_x, col_y, _, _ = data['render_2']\n",
    "\t\trow_x, row_y, _, _ = data['render_4']\n",
    "\texcept KeyError:\n",
    "\t\tprint(f\"Missing base files for rotation {rot}. Skipping.\")\n",
    "\t\tcontinue\n",
    "\n",
    "\t# Calculate shifts\n",
    "\tcol_shift = (col_x - base_x, col_y - base_y)\n",
    "\trow_shift = (row_x - base_x, row_y - base_y)\n",
    "\t# Save computed values back\n",
    "\trotation_data[rot] = {\n",
    "\t\t'base': (base_x, base_y),\n",
    "\t\t'col_shift': col_shift,\n",
    "\t\t'row_shift': row_shift,\n",
    "\t\t'w': w,\n",
    "\t\t'h': h\n",
    "\t}\n",
    "\n",
    "# Function to compute new coordinates given render number and rotation data\n",
    "def compute_new_coords(render_num, rot_info):\n",
    "\t# Render number is expected as integer in 1..9, mapping to a 3x3 grid.\n",
    "\t# grid_x: how many times to add col_shift, grid_y: how many times to add row_shift.\n",
    "\trender_num = int(render_num)\n",
    "\tif render_num > 9:\n",
    "\t\trender_num -= 9 * (render_num // 9) + (render_num // 9 - 1)\n",
    "\tgrid_x = (render_num - 1) % 3\n",
    "\tgrid_y = (render_num - 1) // 3\n",
    "\tbase_x, base_y = rot_info['base']\n",
    "\tcol_shift_x, col_shift_y = rot_info['col_shift']\n",
    "\trow_shift_x, row_shift_y = rot_info['row_shift']\n",
    "\n",
    "\tnew_x = base_x + grid_x * col_shift_x + grid_y * row_shift_x\n",
    "\tnew_y = base_y + grid_x * col_shift_y + grid_y * row_shift_y\n",
    "\treturn new_x, new_y\n",
    "\n",
    "# Now, process each annotation file in the annotations folder\n",
    "img_counter = 0\n",
    "\n",
    "for stage_folder in os.listdir(base_dir_annotations):\n",
    "\tstage_path = os.path.join(base_dir_annotations, stage_folder)\n",
    "\tif not os.path.isdir(stage_path):\n",
    "\t\tcontinue\n",
    "\t# Expect folder name like stage_0, stage_1, etc.\n",
    "\tstage_match = re.match(r'stage_(\\d+)', stage_folder)\n",
    "\tif not stage_match:\n",
    "\t\tcontinue\n",
    "\tclass_id = stage_match.group(1)\n",
    "\tprint(\"hm\")\n",
    "\t# Process each file in the stage folder\n",
    "\tfor fname in os.listdir(stage_path):\n",
    "\t\tannot_match = annot_pattern.match(fname)\n",
    "\t\tprint(fname)\n",
    "\t\tif not annot_match:\n",
    "\t\t\tcontinue\n",
    "\t\tfile_class, render_str, rot = annot_match.groups()\n",
    "\t\t# We ignore file_class (it might be redundant with the folder) and any perm parts\n",
    "\t\tif rot not in rotation_data:\n",
    "\t\t\tprint(f\"Rotation {rot} not found in base data for file {fname}. Skipping.\")\n",
    "\t\t\tcontinue\n",
    "\t\trot_info = rotation_data[rot]\n",
    "\t\t# Compute new x and y using the grid derived from render number\n",
    "\t\tnew_x, new_y = compute_new_coords(render_str, rot_info)\n",
    "\t\tw = rot_info['w']\n",
    "\t\th = rot_info['h']\n",
    "\n",
    "\t\t# Create new annotation line (class from folder, then new_x, new_y, w, h)\n",
    "\t\tnew_line = f\"{class_id} {new_x} {new_y} {w} {h}\\n\"\n",
    "\t\t# Overwrite the file\n",
    "\t\tout_path = os.path.join(stage_path, fname)\n",
    "\t\twith open(out_path, 'w') as f:\n",
    "\t\t\tf.write(new_line)\n",
    "\t\tif img_counter % 1000 == 0:\n",
    "\t\t\tprint(f\"Updated {out_path} with: {new_line.strip()}\")\n",
    "\t\timg_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAM (2.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the SAM model and processor.\n",
    "sam_model = \"facebook/sam2.1-hiera-large\"\n",
    "predictor = SAM2ImagePredictor.from_pretrained(sam_model)\n",
    "def pca_color_feature(feature_map, n_components=3):\n",
    "\t\"\"\"\n",
    "\tApply PCA to a feature map and reduce it to n_components (3 for RGB).\n",
    "\n",
    "\tParameters:\n",
    "\t\tfeature_map: a torch.Tensor of shape (channels, height, width)\n",
    "\t\tn_components: the number of PCA components (default is 3)\n",
    "\n",
    "\tReturns:\n",
    "\t\tA normalized image tensor of shape (height, width, n_components) with values in [0, 1].\n",
    "\t\"\"\"\n",
    "\t# Rearrange feature_map to shape (height, width, channels)\n",
    "\tfm = feature_map.permute(1, 2, 0)   # (H, W, C)\n",
    "\tH, W, C = fm.shape\n",
    "\n",
    "\t# Flatten the spatial dimensions: shape -> (H*W, C)\n",
    "\tfm_flat = fm.reshape(-1, C)\n",
    "\n",
    "\t# Compute PCA using torch.pca_lowrank, reducing to n_components.\n",
    "\t# This returns U, S, V such that fm_flat â U @ diag(S) @ V.T\n",
    "\tU, S, V = torch.pca_lowrank(fm_flat, q=n_components)\n",
    "\n",
    "\t# Project the flattened feature vectors onto the PCA space using V.\n",
    "\t# V has shape (C, n_components), so use:\n",
    "\treduced = torch.matmul(fm_flat, V)  # shape: (H*W, n_components)\n",
    "\n",
    "\t# Reshape the reduced features back to (H, W, n_components)\n",
    "\treduced = reduced.reshape(H, W, n_components)\n",
    "\n",
    "\t# Normalize each PCA channel to [0, 1] for display.\n",
    "\tflat_reduced = reduced.reshape(-1, n_components)\n",
    "\tmin_vals = flat_reduced.min(dim=0)[0]\n",
    "\tmax_vals = flat_reduced.max(dim=0)[0]\n",
    "\tnormalized = (reduced - min_vals) / (max_vals - min_vals + 1e-5)\n",
    "\n",
    "\treturn normalized\n",
    "\n",
    "def display_edges_with_pca(image_path, output_file=None):\n",
    "\t\"\"\"\n",
    "\tLoad an image from disk, extract a feature map via SAM's image encoder,\n",
    "\tand use PCA to reduce the features to 3 channels (RGB). The result\n",
    "\thighlights edges and high-variance features.\n",
    "\n",
    "\tParameters:\n",
    "\timage_path (str): Path to a jpg or png image.\n",
    "\toutput_file (str, optional): Path where the output image will be saved.\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tIf None, the image is displayed.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Load the image and convert it to RGB.\n",
    "\traw_image = Image.open(image_path).convert(\"RGB\")\n",
    "\tpredictor.set_image(raw_image)\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\t# 1 x C x H x W  â take first batch dim\n",
    "\t\tfeature_map = predictor.get_image_embedding()[0]\t # NEW\n",
    "\n",
    "\tpca_result = pca_color_feature(feature_map)\n",
    "\n",
    "\t# Either save the image or show it.\n",
    "\tif output_file is not None:\n",
    "\t\tnp_img = pca_result.cpu().detach().numpy()\n",
    "\t\tnp_img = (np_img * 255).astype(np.uint8)\n",
    "\n",
    "\t\t# Resize PCA result to match original image resolution\n",
    "\t\toriginal_size = raw_image.size  # (width, height)\n",
    "\t\tnp_img_resized = cv2.resize(np_img, original_size, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "\t\t# Convert RGB -> BGR and save with OpenCV\n",
    "\t\tnp_img_bgr = cv2.cvtColor(np_img_resized, cv2.COLOR_RGB2BGR)\n",
    "\t\tcv2.imwrite(output_file, np_img_bgr)\n",
    "\telse:\n",
    "\t\t# Create a figure and display the PCA-reduced feature map.\n",
    "\t\tplt.figure(figsize=(10, 10))\n",
    "\t\tplt.imshow(pca_result.cpu().detach().numpy())\n",
    "\t\tplt.title(\"PCA Reduced Feature Map (Edges Emphasized)\")\n",
    "\t\tplt.axis(\"off\")\n",
    "\n",
    "\t\tplt.show()\n",
    "\n",
    "# Example usage:\n",
    "# To save the output image:\n",
    "# display_edges_with_pca(\"path/to/your/image.jpg\", output_file=\"output_pca_image.png\")\n",
    "# To simply display the image:\n",
    "# display_edges_with_pca(\"path/to/your/image.jpg\")\n",
    "\n",
    "\n",
    "\t\n",
    "# Process images in each stage.\n",
    "img_counter = 0\n",
    "stages = [f\"stage_{i}\" for i in range(N_STAGES)]\n",
    "for stage in stages:\n",
    "\tin_image_folder = os.path.join(base_dir_images, stage)\n",
    "\tin_annotation_folder = os.path.join(base_dir_annotations, stage)\n",
    "\tout_image_folder = os.path.join(out_base_images, stage)\n",
    "\tout_annotation_folder = os.path.join(out_base_annotations, stage)\n",
    "\t\n",
    "\tos.makedirs(out_image_folder, exist_ok=True)\n",
    "\tos.makedirs(out_annotation_folder, exist_ok=True)\n",
    "\t\n",
    "\t# Select image files (filtering out already augmented ones that include \"_translate\").\n",
    "\timage_files = [f for f in os.listdir(in_image_folder)\n",
    "\t\t\t\t   if f.lower().endswith(\".jpg\") and \"_sam\" not in f]\n",
    "\t\n",
    "\tfor filename in image_files:\n",
    "\t\timage_path = os.path.join(in_image_folder, filename)\n",
    "\t\tbase_filename = os.path.splitext(filename)[0]\n",
    "\t\tannotation_filename = base_filename + \".txt\"\n",
    "\t\tannotation_path = os.path.join(in_annotation_folder, annotation_filename)\n",
    "\t\t\n",
    "\t\tout_image_filename = f\"{base_filename}_sam.jpg\"\n",
    "\t\tout_annotation_filename = f\"{base_filename}_sam.txt\"\n",
    "\t\tout_image_path = os.path.join(out_image_folder, out_image_filename)\n",
    "\t\tout_annotation_path = os.path.join(out_annotation_folder, out_annotation_filename)\n",
    "\t\t\n",
    "\t\tif not os.path.exists(out_image_path):\n",
    "\t\t\t# Apply the random translation augmentation.\n",
    "\t\t\tdisplay_edges_with_pca(image_path, out_image_path)\n",
    "\n",
    "\t\t\t# Copy annotation unchanged.\n",
    "\t\t\tif os.path.exists(annotation_path):\n",
    "\t\t\t\twith open(annotation_path, 'r') as f:\n",
    "\t\t\t\t\tcontent = f.read()\n",
    "\t\t\t\tout_annotation_path = os.path.join(out_annotation_folder, f\"{base_filename}_sam.txt\")\n",
    "\t\t\t\twith open(out_annotation_path, 'w') as f:\n",
    "\t\t\t\t\tf.write(content)\n",
    "\t\t\tif img_counter % 1000 == 0:\n",
    "\t\t\t\tprint(f\"Processed sam augmentation: {out_image_path}\")\n",
    "\t\t\timg_counter += 1\n",
    "\n",
    "# Example usage:\n",
    "# Replace the path below with the location of your local image.\n",
    "# display_edges_with_pca(\"path/to/your/image.jpg\")\n",
    "\n",
    "# After you are done using predictor:\n",
    "del predictor  # delete the object\n",
    "if device == \"cuda\":\n",
    "\ttorch.cuda.empty_cache()\n",
    "\ttorch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tint patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tint_image_with_patches(image_path, annotation_path, out_image_path, out_annotation_path, patch_size=64, alpha=0.3):\n",
    "\timg = cv2.imread(image_path)\n",
    "\tif img is None:\n",
    "\t\tprint(\"Failed to read image:\", image_path)\n",
    "\t\treturn\n",
    "\t\n",
    "\th, w, _ = img.shape\n",
    "\ttinted_img = img.copy()\n",
    "\n",
    "\t# Iterate over image in 64x64 patches\n",
    "\tfor y in range(0, h, patch_size):\n",
    "\t\tfor x in range(0, w, patch_size):\n",
    "\t\t\t# Create random color\n",
    "\t\t\tcolor = np.array([random.randint(0, 255) for _ in range(3)], dtype=np.uint8)\n",
    "\t\t\t# Create a solid patch with the color\n",
    "\t\t\tcolor_patch = np.full((patch_size, patch_size, 3), color, dtype=np.uint8)\n",
    "\n",
    "\t\t\t# Define the region to blend\n",
    "\t\t\tend_y = min(y + patch_size, h)\n",
    "\t\t\tend_x = min(x + patch_size, w)\n",
    "\n",
    "\t\t\t# Blend original image and color patch\n",
    "\t\t\tregion = tinted_img[y:end_y, x:end_x]\n",
    "\t\t\tblended = cv2.addWeighted(region, 1 - alpha, color_patch[:end_y - y, :end_x - x], alpha, 0)\n",
    "\t\t\ttinted_img[y:end_y, x:end_x] = blended\n",
    "\n",
    "\tcv2.imwrite(out_image_path, tinted_img)\n",
    "\n",
    "\t# Copy the annotation unchanged\n",
    "\tif os.path.exists(annotation_path):\n",
    "\t\twith open(annotation_path, 'r') as f:\n",
    "\t\t\tcontent = f.read()\n",
    "\t\twith open(out_annotation_path, 'w') as f:\n",
    "\t\t\tf.write(content)\n",
    "\n",
    "tag = \"colorpatch\"\n",
    "img_counter = 0\n",
    "stages = [f\"stage_{i}\" for i in range(N_STAGES)]\n",
    "\n",
    "for stage in stages:\n",
    "\tin_image_folder = os.path.join(base_dir_images, stage)\n",
    "\tin_annotation_folder = os.path.join(base_dir_annotations, stage)\n",
    "\tout_image_folder = os.path.join(out_base_images, stage)\n",
    "\tout_annotation_folder = os.path.join(out_base_annotations, stage)\n",
    "\tos.makedirs(out_image_folder, exist_ok=True)\n",
    "\tos.makedirs(out_annotation_folder, exist_ok=True)\n",
    "\t\n",
    "\t# Cache the list of original image files (skip those already augmented)\n",
    "\timage_files = [f for f in os.listdir(in_image_folder)\n",
    "\t\t\t\t\t\tif f.lower().endswith(\".jpg\") and \"_colorpatch\" not in f]\n",
    "\t\n",
    "\tdef process_image(filename):\n",
    "\t\timage_path = os.path.join(in_image_folder, filename)\n",
    "\t\tbase_filename = os.path.splitext(filename)[0]\n",
    "\t\tannotation_filename = base_filename + \".txt\"\n",
    "\t\tannotation_path = os.path.join(in_annotation_folder, annotation_filename)\n",
    "\n",
    "\t\tout_image_filename = f\"{base_filename}_{tag}.jpg\"\n",
    "\t\tout_annotation_filename = f\"{base_filename}_{tag}.txt\"\n",
    "\t\tout_image_path = os.path.join(out_image_folder, out_image_filename)\n",
    "\t\tout_annotation_path = os.path.join(out_annotation_folder, out_annotation_filename)\n",
    "\n",
    "\t\tif not os.path.exists(out_image_path):\n",
    "\t\t\ttint_image_with_patches(image_path, annotation_path, out_image_path, out_annotation_path)\n",
    "\n",
    "\twith ProcessPoolExecutor(max_workers=8) as executor:\n",
    "\t\texecutor.map(process_image, image_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def adjust_brightness_and_copy_annotation(image_path, annotation_path, out_image_path, out_annotation_path, factor):\n",
    "\timg = cv2.imread(image_path)\n",
    "\tif img is None:\n",
    "\t\tprint(\"Failed to read image:\", image_path)\n",
    "\t\treturn\n",
    "\t# Adjust brightness/darkness using cv2.convertScaleAbs.\n",
    "\tadjusted_img = cv2.convertScaleAbs(img, alpha=factor, beta=0)\n",
    "\tcv2.imwrite(out_image_path, adjusted_img)\n",
    "\t\n",
    "\t# Copy the annotation unchanged.\n",
    "\tif os.path.exists(annotation_path):\n",
    "\t\twith open(annotation_path, 'r') as f:\n",
    "\t\t\tcontent = f.read()\n",
    "\t\twith open(out_annotation_path, 'w') as f:\n",
    "\t\t\tf.write(content)\n",
    "\n",
    "img_counter = 0\n",
    "stages = [f\"stage_{i}\" for i in range(N_STAGES)]\n",
    "# Define brightness augmentation factors.\n",
    "brightness_aug = {\n",
    "\t\"bright125\": 1.5,\n",
    "\t\"dark075\": 0.5,\n",
    "}\n",
    "\n",
    "for stage in stages:\n",
    "\tin_image_folder = os.path.join(base_dir_images, stage)\n",
    "\tin_annotation_folder = os.path.join(base_dir_annotations, stage)\n",
    "\tout_image_folder = os.path.join(out_base_images, stage)\n",
    "\tout_annotation_folder = os.path.join(out_base_annotations, stage)\n",
    "\tos.makedirs(out_image_folder, exist_ok=True)\n",
    "\tos.makedirs(out_annotation_folder, exist_ok=True)\n",
    "\t\n",
    "\t# Cache the list of original image files (skip those already augmented)\n",
    "\timage_files = [f for f in os.listdir(in_image_folder)\n",
    "\t\t\t\t\t\tif f.lower().endswith(\".jpg\") and \"_bright\" not in f and \"_dark\" not in f]\n",
    "\t\n",
    "\tdef process_brightness_aug(task):\n",
    "\t\tfilename, tag, factor = task\n",
    "\t\timage_path = os.path.join(in_image_folder, filename)\n",
    "\t\tbase_filename = os.path.splitext(filename)[0]\n",
    "\t\tannotation_filename = base_filename + \".txt\"\n",
    "\t\tannotation_path = os.path.join(in_annotation_folder, annotation_filename)\n",
    "\n",
    "\t\tout_image_filename = f\"{base_filename}_{tag}.jpg\"\n",
    "\t\tout_annotation_filename = f\"{base_filename}_{tag}.txt\"\n",
    "\t\tout_image_path = os.path.join(out_image_folder, out_image_filename)\n",
    "\t\tout_annotation_path = os.path.join(out_annotation_folder, out_annotation_filename)\n",
    "\n",
    "\t\tif not os.path.exists(out_image_path):\n",
    "\t\t\tadjust_brightness_and_copy_annotation(image_path, annotation_path, out_image_path, out_annotation_path, factor)\n",
    "\n",
    "\t# Prepare all (filename, tag, factor) tasks\n",
    "\ttasks = [(filename, tag, factor) for filename in image_files for tag, factor in brightness_aug.items()]\n",
    "\n",
    "\t# Parallel execution\n",
    "\twith ThreadPoolExecutor(max_workers=16) as executor:\n",
    "\t\texecutor.map(process_brightness_aug, tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obscure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each image in the base directories, read the corresponding annotation file,\n",
    "apply the obscure augmentation by drawing a random, smaller rectangle within each bounding box,\n",
    "and then save the resulting image and a copy of the annotation file in the output directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def obscure_object_in_image(image, annotation_path):\n",
    "\t\"\"\"\n",
    "\tFor each bounding box (in YOLO format) in the given annotation file,\n",
    "\tdraw a randomly sized and rotated rectangle inside the bounding box to obscure it.\n",
    "\t\"\"\"\n",
    "\tH, W = image.shape[:2]\n",
    "\tif not os.path.exists(annotation_path):\n",
    "\t\treturn image\n",
    "\n",
    "\twith open(annotation_path, 'r') as f:\n",
    "\t\tlines = f.readlines()\n",
    "\n",
    "\tfor line in lines:\n",
    "\t\tparts = line.strip().split()\n",
    "\t\tif len(parts) != 5:\n",
    "\t\t\tcontinue\n",
    "\t\tcls, cx, cy, bw, bh = parts\n",
    "\t\tcx, cy, bw, bh = float(cx), float(cy), float(bw), float(bh)\n",
    "\t\t\n",
    "\t\t# Calculate bounding box pixel coordinates.\n",
    "\t\tbox_w = bw * W\n",
    "\t\tbox_h = bh * H\n",
    "\t\tbox_x1 = cx * W - box_w / 2\n",
    "\t\tbox_y1 = cy * H - box_h / 2\n",
    "\t\tbox_x2 = box_x1 + box_w\n",
    "\t\tbox_y2 = box_y1 + box_h\n",
    "\n",
    "\t\t# Determine random scale factors for the inner (obscuring) rectangle.\n",
    "\t\tscale_w = random.uniform(0.1, 0.4)\n",
    "\t\tscale_h = random.uniform(0.1, 0.4)\n",
    "\t\trect_w = box_w * scale_w\n",
    "\t\trect_h = box_h * scale_h\n",
    "\n",
    "\t\t# Choose a random center for the inner rectangle ensuring it fits within the box.\n",
    "\t\tmin_cx = box_x1 + rect_w / 2\n",
    "\t\tmax_cx = box_x2 - rect_w / 2\n",
    "\t\tmin_cy = box_y1 + rect_h / 2\n",
    "\t\tmax_cy = box_y2 - rect_h / 2\n",
    "\t\tif max_cx < min_cx or max_cy < min_cy:\n",
    "\t\t\trect_center_x = (box_x1 + box_x2) / 2\n",
    "\t\t\trect_center_y = (box_y1 + box_y2) / 2\n",
    "\t\telse:\n",
    "\t\t\trect_center_x = random.uniform(min_cx, max_cx)\n",
    "\t\t\trect_center_y = random.uniform(min_cy, max_cy)\n",
    "\n",
    "\t\t# Determine a random rotation angle.\n",
    "\t\tangle = random.uniform(0, 360)\n",
    "\n",
    "\t\t# Create the rotated rectangle and get its corner points.\n",
    "\t\trect = ((rect_center_x, rect_center_y), (rect_w, rect_h), angle)\n",
    "\t\tbox_points = cv2.boxPoints(rect)\n",
    "\t\tbox_points = box_points.astype(np.int32)\n",
    "\n",
    "\t\t# Draw a filled rectangle (using black color to obscure).\n",
    "\t\tcv2.fillPoly(image, [box_points], (0, 0, 0))\n",
    "\t\t\n",
    "\treturn image\n",
    "\n",
    "\n",
    "img_counter = 0\n",
    "stages = [f\"stage_{i}\" for i in range(8)]\n",
    "\n",
    "for stage in stages:\n",
    "\tin_image_folder = os.path.join(base_dir_images, stage)\n",
    "\tin_annotation_folder = os.path.join(base_dir_annotations, stage)\n",
    "\tout_image_folder = os.path.join(out_base_images, stage)\n",
    "\tout_annotation_folder = os.path.join(out_base_annotations, stage)\n",
    "\tos.makedirs(out_image_folder, exist_ok=True)\n",
    "\tos.makedirs(out_annotation_folder, exist_ok=True)\n",
    "\t\n",
    "\t# Cache the list of original image files (skip those already augmented).\n",
    "\timage_files = [f for f in os.listdir(in_image_folder)\n",
    "\t\t\t\t\t\tif f.lower().endswith(\".jpg\") and \"_obscure\" not in f]\n",
    "\t\n",
    "\tdef process_obscure(filename):\n",
    "\t\timage_path = os.path.join(in_image_folder, filename)\n",
    "\t\tbase_filename = os.path.splitext(filename)[0]\n",
    "\t\tannotation_filename = base_filename + \".txt\"\n",
    "\t\tannotation_path = os.path.join(in_annotation_folder, annotation_filename)\n",
    "\t\tout_image_filename = f\"{base_filename}_obscure.jpg\"\n",
    "\t\tout_image_path = os.path.join(out_image_folder, out_image_filename)\n",
    "\n",
    "\t\tif not os.path.exists(out_image_path):\n",
    "\t\t\timg = cv2.imread(image_path)\n",
    "\t\t\tif img is None:\n",
    "\t\t\t\tprint(\"Failed to read image:\", image_path)\n",
    "\t\t\t\treturn  # Skip if image reading failed\n",
    "\n",
    "\t\t\t# Obscure objects in the image using their bounding boxes\n",
    "\t\t\tobscured_img = obscure_object_in_image(img, annotation_path)\n",
    "\t\t\tcv2.imwrite(out_image_path, obscured_img)\n",
    "\n",
    "\t\t\t# Copy annotation unchanged\n",
    "\t\t\tif os.path.exists(annotation_path):\n",
    "\t\t\t\tout_annotation_path = os.path.join(out_annotation_folder, f\"{base_filename}_obscure.txt\")\n",
    "\t\t\t\twith open(annotation_path, 'r') as f_in, open(out_annotation_path, 'w') as f_out:\n",
    "\t\t\t\t\tf_out.write(f_in.read())\n",
    "\n",
    "\t\t\treturn out_image_path  # For optional progress tracking\n",
    "\n",
    "\t# Prepare list of filenames to process\n",
    "\ttasks = [f for f in image_files if not os.path.exists(os.path.join(out_image_folder, os.path.splitext(f)[0] + \"_obscure.jpg\"))]\n",
    "\n",
    "\t# Parallel execution\n",
    "\twith ThreadPoolExecutor(max_workers=16) as executor:\n",
    "\t\tfor idx, result in enumerate(executor.map(process_obscure, tasks)):\n",
    "\t\t\tif result is not None and idx % 1000 == 0:\n",
    "\t\t\t\tprint(f\"Processed obscure augmentation: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def random_scale_and_crop_annotation(image_path, annotation_path, out_image_path, out_annotation_path, scale_range=(1.0, 2.0)):\n",
    "\t\"\"\"\n",
    "\tScales the image by a random factor and then crops the center region to \n",
    "\trestore the original image size. Adjusts YOLO annotations accordingly.\n",
    "\t\"\"\"\n",
    "\t# Read the original image.\n",
    "\timg = cv2.imread(image_path)\n",
    "\tif img is None:\n",
    "\t\tprint(\"Failed to read image:\", image_path)\n",
    "\t\treturn\n",
    "\t\n",
    "\toriginal_h, original_w = img.shape[:2]\n",
    "\n",
    "\t# Randomly select a scale factor.\n",
    "\tscale_factor = random.uniform(scale_range[0], scale_range[1])\n",
    "\t\n",
    "\t# Compute new dimensions for the scaled image.\n",
    "\tnew_w = int(original_w * scale_factor)\n",
    "\tnew_h = int(original_h * scale_factor)\n",
    "\t\n",
    "\t# Resize (scale) the image.\n",
    "\tscaled_img = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "\t\n",
    "\t# Ensure the scaled image is large enough for the crop.\n",
    "\tif new_w < original_w or new_h < original_h:\n",
    "\t\tprint(\"Scaled image is smaller than original; adjust scale_range or add padding.\")\n",
    "\t\treturn\n",
    "\n",
    "\t# Compute crop offsets to extract the center region.\n",
    "\tx_offset = (new_w - original_w) // 2\n",
    "\ty_offset = (new_h - original_h) // 2\n",
    "\t\n",
    "\t# Crop the center region.\n",
    "\tcropped_img = scaled_img[y_offset:y_offset + original_h, x_offset:x_offset + original_w]\n",
    "\t\n",
    "\t# Write the augmented image.\n",
    "\tcv2.imwrite(out_image_path, cropped_img)\n",
    "\t\n",
    "\t# Read and update the annotation file.\n",
    "\t# Assumption: annotations are in YOLO normalized format.\n",
    "\tadjusted_lines = []\n",
    "\tif os.path.exists(annotation_path):\n",
    "\t\twith open(annotation_path, 'r') as file:\n",
    "\t\t\tfor line in file:\n",
    "\t\t\t\tparts = line.strip().split()\n",
    "\t\t\t\tif len(parts) == 5:\n",
    "\t\t\t\t\tclass_id, cx, cy, bw, bh = parts\n",
    "\t\t\t\t\tcx = float(cx)\n",
    "\t\t\t\t\tcy = float(cy)\n",
    "\t\t\t\t\tbw = float(bw)\n",
    "\t\t\t\t\tbh = float(bh)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Convert normalized coordinates to absolute pixel coordinates in the original image.\n",
    "\t\t\t\t\tabs_cx = cx * original_w\n",
    "\t\t\t\t\tabs_cy = cy * original_h\n",
    "\t\t\t\t\tabs_bw = bw * original_w\n",
    "\t\t\t\t\tabs_bh = bh * original_h\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Apply scaling.\n",
    "\t\t\t\t\tscaled_cx = abs_cx * scale_factor\n",
    "\t\t\t\t\tscaled_cy = abs_cy * scale_factor\n",
    "\t\t\t\t\tscaled_bw = abs_bw * scale_factor\n",
    "\t\t\t\t\tscaled_bh = abs_bh * scale_factor\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Adjust for cropping by subtracting the offsets.\n",
    "\t\t\t\t\tcropped_cx = scaled_cx - x_offset\n",
    "\t\t\t\t\tcropped_cy = scaled_cy - y_offset\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# (Optional) If a bounding box falls partially outside,\n",
    "\t\t\t\t\t# you can clip the box here. For simplicity, we assume\n",
    "\t\t\t\t\t# the boxes are fully contained.\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Convert back to normalized coordinates with respect to the output image dimensions.\n",
    "\t\t\t\t\tnew_cx = cropped_cx / original_w\n",
    "\t\t\t\t\tnew_cy = cropped_cy / original_h\n",
    "\t\t\t\t\tnew_bw = scaled_bw / original_w\n",
    "\t\t\t\t\tnew_bh = scaled_bh / original_h\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Write the new annotation line.\n",
    "\t\t\t\t\tadjusted_line = f\"{class_id} {new_cx:.6f} {new_cy:.6f} {new_bw:.6f} {new_bh:.6f}\\n\"\n",
    "\t\t\t\t\tadjusted_lines.append(adjusted_line)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t# For lines that do not match the expected format, copy them as is.\n",
    "\t\t\t\t\tadjusted_lines.append(line)\n",
    "\t\t# Save the adjusted annotations.\n",
    "\t\twith open(out_annotation_path, 'w') as file:\n",
    "\t\t\tfile.writelines(adjusted_lines)\n",
    "\t\n",
    "\n",
    "stages = [f\"stage_{i}\" for i in range(N_STAGES)]\n",
    "img_counter = 0\n",
    "\n",
    "for stage in stages:\n",
    "\tin_image_folder = os.path.join(base_dir_images, stage)\n",
    "\tin_annotation_folder = os.path.join(base_dir_annotations, stage)\n",
    "\tout_image_folder = os.path.join(out_base_images, stage)\n",
    "\tout_annotation_folder = os.path.join(out_base_annotations, stage)\n",
    "\t\n",
    "\tos.makedirs(out_image_folder, exist_ok=True)\n",
    "\tos.makedirs(out_annotation_folder, exist_ok=True)\n",
    "\t\n",
    "\timage_files = [f for f in os.listdir(in_image_folder)\n",
    "\t\t\t\t   if f.lower().endswith(\".jpg\") and \"_scale\" not in f]\n",
    "\t\n",
    "\tdef process_scale(filename):\n",
    "\t\timage_path = os.path.join(in_image_folder, filename)\n",
    "\t\tbase_filename = os.path.splitext(filename)[0]\n",
    "\t\tannotation_filename = base_filename + \".txt\"\n",
    "\t\tannotation_path = os.path.join(in_annotation_folder, annotation_filename)\n",
    "\n",
    "\t\tout_image_filename = f\"{base_filename}_scale.jpg\"\n",
    "\t\tout_annotation_filename = f\"{base_filename}_scale.txt\"\n",
    "\t\tout_image_path = os.path.join(out_image_folder, out_image_filename)\n",
    "\t\tout_annotation_path = os.path.join(out_annotation_folder, out_annotation_filename)\n",
    "\n",
    "\t\tif not os.path.exists(out_image_path):\n",
    "\t\t\trandom_scale_and_crop_annotation(image_path, annotation_path, out_image_path, out_annotation_path)\n",
    "\t\t\treturn out_image_path  # For optional progress tracking\n",
    "\n",
    "\t# Prepare only tasks where output doesn't exist yet\n",
    "\ttasks = [f for f in image_files if not os.path.exists(os.path.join(out_image_folder, os.path.splitext(f)[0] + \"_scale.jpg\"))]\n",
    "\n",
    "\t# Parallel execution\n",
    "\twith ThreadPoolExecutor(max_workers=16) as executor:\n",
    "\t\tfor idx, result in enumerate(executor.map(process_scale, tasks)):\n",
    "\t\t\tif result is not None and idx % 1000 == 0:\n",
    "\t\t\t\tprint(f\"Processed scale augmentation: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def random_translate_and_update_annotation(image_path, annotation_path, out_image_path, out_annotation_path):\n",
    "\t\"\"\"\n",
    "\tApplies a random translation to the image and updates the annotation coordinates accordingly.\n",
    "\t\n",
    "\tThe translation is random:\n",
    "\t  - x direction: uniformly chosen from -1/4 to +1/4 of the image width.\n",
    "\t  - y direction: uniformly chosen from -1/4 to +1/4 of the image height.\n",
    "\t\n",
    "\tThe annotation is assumed to be in a simple format per line:\n",
    "\t\tclass_id x_center y_center box_width box_height\n",
    "\twhere x_center and y_center are given as pixel coordinates.\n",
    "\t\"\"\"\n",
    "\t# Read the image.\n",
    "\timg = cv2.imread(image_path)\n",
    "\tif img is None:\n",
    "\t\tprint(\"Failed to read image:\", image_path)\n",
    "\t\treturn\n",
    "\t\n",
    "\t# Get image dimensions.\n",
    "\theight, width = img.shape[:2]\n",
    "\t\n",
    "\t# Determine maximum translation amounts.\n",
    "\tmax_tx = width / 4.0  # maximum translation in x direction.\n",
    "\tmax_ty = height / 4.0  # maximum translation in y direction.\n",
    "\t\n",
    "\t# Randomly select translation values (they can be negative or positive).\n",
    "\ttx = random.uniform(-max_tx, max_tx)\n",
    "\tty = random.uniform(-max_ty, max_ty)\n",
    "\t\n",
    "\t# Define translation matrix.\n",
    "\tM = np.float32([[1, 0, tx],\n",
    "\t\t\t\t\t[0, 1, ty]])\n",
    "\t\n",
    "\t# Apply translation using cv2.warpAffine. The resulting image is kept at original dimensions.\n",
    "\ttranslated_img = cv2.warpAffine(img, M, (width, height))\n",
    "\tcv2.imwrite(out_image_path, translated_img)\n",
    "\t\n",
    "\t# If the annotation file exists, update the coordinates.\n",
    "\tif os.path.exists(annotation_path):\n",
    "\t\tnew_lines = []\n",
    "\t\twith open(annotation_path, 'r') as f:\n",
    "\t\t\tfor line in f:\n",
    "\t\t\t\tparts = line.strip().split()\n",
    "\t\t\t\t# Check if the line has the expected format.\n",
    "\t\t\t\tif len(parts) == 5:\n",
    "\t\t\t\t\tclass_id, x_center, y_center, box_w, box_h = parts\n",
    "\t\t\t\t\t# Update the center coordinates by adding the translation offsets.\n",
    "\t\t\t\t\tx_center = (float(x_center) + (tx/width))\n",
    "\t\t\t\t\ty_center = (float(y_center) + (ty/height))\n",
    "\t\t\t\t\tnew_line = f\"{class_id} {x_center:.2f} {y_center:.2f} {box_w} {box_h}\"\n",
    "\t\t\t\t\tnew_lines.append(new_line)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t# If the line doesn't match the expected format, copy it unchanged.\n",
    "\t\t\t\t\tnew_lines.append(line.strip())\n",
    "\t\twith open(out_annotation_path, 'w') as f:\n",
    "\t\t\tf.write(\"\\n\".join(new_lines))\n",
    "\t\n",
    "\t\n",
    "# Process images in each stage.\n",
    "stages = [f\"stage_{i}\" for i in range(N_STAGES)]\n",
    "img_counter = 0\n",
    "\n",
    "for stage in stages:\n",
    "\tin_image_folder = os.path.join(base_dir_images, stage)\n",
    "\tin_annotation_folder = os.path.join(base_dir_annotations, stage)\n",
    "\tout_image_folder = os.path.join(out_base_images, stage)\n",
    "\tout_annotation_folder = os.path.join(out_base_annotations, stage)\n",
    "\t\n",
    "\tos.makedirs(out_image_folder, exist_ok=True)\n",
    "\tos.makedirs(out_annotation_folder, exist_ok=True)\n",
    "\t\n",
    "\t# Select image files (filtering out already augmented ones that include \"_translate\").\n",
    "\timage_files = [f for f in os.listdir(in_image_folder)\n",
    "\t\t\t\t   if f.lower().endswith(\".jpg\") and \"_translate\" not in f]\n",
    "\t\n",
    "\tdef process_translation(filename):\n",
    "\t\timage_path = os.path.join(in_image_folder, filename)\n",
    "\t\tbase_filename = os.path.splitext(filename)[0]\n",
    "\t\tannotation_filename = base_filename + \".txt\"\n",
    "\t\tannotation_path = os.path.join(in_annotation_folder, annotation_filename)\n",
    "\n",
    "\t\tout_image_filename = f\"{base_filename}_translate.jpg\"\n",
    "\t\tout_annotation_filename = f\"{base_filename}_translate.txt\"\n",
    "\t\tout_image_path = os.path.join(out_image_folder, out_image_filename)\n",
    "\t\tout_annotation_path = os.path.join(out_annotation_folder, out_annotation_filename)\n",
    "\n",
    "\t\tif not os.path.exists(out_image_path):\n",
    "\t\t\trandom_translate_and_update_annotation(image_path, annotation_path, out_image_path, out_annotation_path)\n",
    "\t\t\treturn out_image_path  # For optional progress tracking\n",
    "\n",
    "\t# Prepare only tasks where output doesn't exist yet\n",
    "\ttasks = [f for f in image_files if not os.path.exists(os.path.join(out_image_folder, os.path.splitext(f)[0] + \"_translate.jpg\"))]\n",
    "\n",
    "\t# Parallel execution\n",
    "\twith ThreadPoolExecutor(max_workers=16) as executor:\n",
    "\t\tfor idx, result in enumerate(executor.map(process_translation, tasks)):\n",
    "\t\t\tif result is not None and idx % 1000 == 0:\n",
    "\t\t\t\tprint(f\"Processed translation augmentation: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def show_image_with_bbox(filename, bbox):\n",
    "# \t\"\"\"\n",
    "# \tDisplay an image with a bounding box drawn on it.\n",
    "\n",
    "# \tParameters:\n",
    "# \t- filename: str, path to the image file.\n",
    "# \t- bbox: tuple of (class_id, x_center, y_center, width, height) in YOLO format (all normalized).\n",
    "# \t\"\"\"\n",
    "# \t# Load the image\n",
    "# \timage = cv2.imread(filename)\n",
    "# \tif image is None:\n",
    "# \t\tprint(f\"Error: Unable to load image at {filename}\")\n",
    "# \t\treturn\n",
    "\n",
    "# \t# Get image dimensions (height, width)\n",
    "# \th, w, _ = image.shape\n",
    "\n",
    "# \t# Convert YOLO normalized coordinates to absolute pixel values\n",
    "# \tclass_id, x_center, y_center, box_width, box_height = bbox\n",
    "# \tx_center_pixel = x_center * w\n",
    "# \ty_center_pixel = y_center * h\n",
    "# \tbox_width_pixel = box_width * w\n",
    "# \tbox_height_pixel = box_height * h\n",
    "\n",
    "# \t# Calculate top-left and bottom-right coordinates of the bounding box\n",
    "# \tx1 = int(x_center_pixel - box_width_pixel / 2)\n",
    "# \ty1 = int(y_center_pixel - box_height_pixel / 2)\n",
    "# \tx2 = int(x_center_pixel + box_width_pixel / 2)\n",
    "# \ty2 = int(y_center_pixel + box_height_pixel / 2)\n",
    "\n",
    "# \t# Convert image color from BGR (OpenCV default) to RGB for matplotlib\n",
    "# \timage_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# \t# Draw the bounding box (red color, thickness 2)\n",
    "# \tcv2.rectangle(image_rgb, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "\n",
    "# \t# Optionally add the class id as text above the bounding box\n",
    "# \tlabel = str(int(class_id))\n",
    "# \tcv2.putText(image_rgb, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "\n",
    "# \t# Display the image with matplotlib\n",
    "# \tplt.figure(figsize=(8, 8))\n",
    "# \tplt.imshow(image_rgb)\n",
    "# \tplt.axis('off')\n",
    "# \tplt.title(\"Image with Bounding Box\")\n",
    "# \tplt.show()\n",
    "\n",
    "# # Example usage\n",
    "# # 0.173398437\n",
    "# if __name__ == \"__main__\":\n",
    "# \timage_filename = f\"{base_dir_images}/stage_0/stage_0_case_render_4_rot20.jpg\"  # Replace with your image file path\n",
    "# \tbounding_box_filename = f\"{base_dir_annotations}stage_0/stage_0_case_render_4_rot20.txt\"\n",
    "# \tfor stage in os.listdir(base_dir_annotations):\n",
    "# \t\tif stage != \"stage_1\":\n",
    "# \t\t\tcontinue\n",
    "# \t\tfor filename in os.listdir(os.path.join(base_dir_images, stage))[1000:1100]:\n",
    "# \t\t\tif filename.endswith(\".jpg\"):\n",
    "# \t\t\t\timage_filename = os.path.join(base_dir_images, stage, filename)\n",
    "# \t\t\t\tbounding_box_filename = os.path.join(base_dir_annotations, stage, filename.replace(\".jpg\", \".txt\"))\n",
    "# \t\t\t\twith open(bounding_box_filename, 'r') as f:\n",
    "# \t\t\t\t\tbbox_info = f.read().strip().split()\n",
    "# \t\t\t\t\tbbox_info = (0, float(bbox_info[1]), float(bbox_info[2]), float(bbox_info[3]), float(bbox_info[4]))\n",
    "# \t\t\t\t\tprint(bounding_box_filename)\n",
    "# \t\t\t\t\tshow_image_with_bbox(image_filename, bbox_info)\n",
    "\n",
    "# # 1: x 0.317216797 0.670823568 0.5 0.51\n",
    "# # 2: x 0.490615234 0.670823568 0.5 0.51\n",
    "# # 3: x 0.664013671 0.670823568 0.5 0.51\n",
    "# # 4: x 0.317216797 0.497425131 0.5 0.51\n",
    "# # 5: x 0.490615234 0.497425131 0.5 0.51\n",
    "# # 6: x 0.664013671 0.497425131 0.5 0.51\n",
    "# # 7: x 0.317216797 0.324026694 0.5 0.51\n",
    "# # 8: x 0.490615234 0.324026694 0.5 0.51\n",
    "# # 9: x 0.664013671 0.324026694 0.5 0.51\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rename stuff from roboflow (Not used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import re\n",
    "\n",
    "# def rename_files_in_folder(folder_path):\n",
    "# \tpattern = re.compile(r\"(.*)_jpg\\.rf\\.[a-f0-9]+\\.jpg$\")\n",
    "\n",
    "# \tfor filename in os.listdir(folder_path):\n",
    "# \t\tif filename.endswith(\".txt\"):\n",
    "# \t\t\tmatch = pattern.match(filename)\n",
    "# \t\t\tif match:\n",
    "# \t\t\t\tnew_filename = f\"{match.group(1)}.txt\"\n",
    "# \t\t\t\told_path = os.path.join(folder_path, filename)\n",
    "# \t\t\t\tnew_path = os.path.join(folder_path, new_filename)\n",
    "# \t\t\t\tos.rename(old_path, new_path)\n",
    "# \t\t\t\tprint(f\"Renamed: {filename} -> {new_filename}\")\n",
    "\n",
    "# # Example usage\n",
    "# folder = \"/Users/georgye/Documents/repos/ethz/dslab25/obj_detection/preprocessing/stage_0/images\"\n",
    "# rename_files_in_folder(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import re\n",
    "# import shutil\n",
    "\n",
    "# def rename_and_duplicate(folder_path):\n",
    "# \t# Regex pattern to match files like stage_x_case_render_n.txt\n",
    "# \tpattern = re.compile(r\"^(stage_\\d+)_case_render_(\\d+)\\.txt$\")\n",
    "\t\n",
    "# \tfor filename in os.listdir(folder_path):\n",
    "# \t\tmatch = pattern.match(filename)\n",
    "# \t\tif match:\n",
    "# \t\t\tstage, render = match.groups()\n",
    "# \t\t\toriginal_path = os.path.join(folder_path, filename)\n",
    "\t\t\t\n",
    "# \t\t\t# New filenames\n",
    "# \t\t\tnew_name_var_0 = f\"{stage}_var_0_case_render_{render}.txt\"\n",
    "# \t\t\tnew_name_var_1 = f\"{stage}_var_1_case_render_{render}.txt\"\n",
    "# \t\t\tpath_var_0 = os.path.join(folder_path, new_name_var_0)\n",
    "# \t\t\tpath_var_1 = os.path.join(folder_path, new_name_var_1)\n",
    "\n",
    "# \t\t\t# Rename original file to var_0\n",
    "# \t\t\tos.rename(original_path, path_var_0)\n",
    "# \t\t\tprint(f\"Renamed: {filename} -> {new_name_var_0}\")\n",
    "\t\t\t\n",
    "# \t\t\t# Duplicate to var_1\n",
    "# \t\t\tshutil.copy(path_var_0, path_var_1)\n",
    "# \t\t\tprint(f\"Duplicated: {new_name_var_0} -> {new_name_var_1}\")\n",
    "# rename_and_duplicate('/Users/georgye/Documents/repos/ethz/dslab25/assets/vacuum_pump/rendered/anno/stage_6')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dslab_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
