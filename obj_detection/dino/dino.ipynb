{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch.multiprocessing as mp\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from safetensors.torch import load_file as load_safetensors\n",
    "from transformers import AutoImageProcessor, TrainingArguments, Trainer\n",
    "from utils import DINOv2Dataset, DINOv2Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From YOLO to COCO annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = os.getcwd().split('dslab25')[0] + 'dslab25/'\n",
    "root_dir = repo_dir + \"training/vacuum_pump\"\n",
    "image_dir = os.path.join(root_dir, \"images/augmented\")\n",
    "label_dir = os.path.join(root_dir, \"annotation/augmented\")\n",
    "coco_path = os.path.join(root_dir, \"coco_annotations.json\")\n",
    "\n",
    "dino_dir = os.path.join(repo_dir, \"obj_detection/dino\") \n",
    "training_dir = os.path.join(repo_dir, \"training/vacuum_pump\")\n",
    "pretrained_model = \"facebook/dinov2-with-registers-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect category mapping\n",
    "stage_folders = [f\"stage_{i}\" for i in range(8)]\n",
    "category_mapping = {name: i for i, name in enumerate(stage_folders)}  # name -> ID\n",
    "categories = [{\"id\": i, \"name\": name} for name, i in category_mapping.items()]\n",
    "\n",
    "# Initialize COCO structure\n",
    "coco_output = {\n",
    "\t\"images\": [],\n",
    "\t\"annotations\": [],\n",
    "\t\"categories\": categories\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from PIL import Image\n",
    "\n",
    "def process_stage(args):\n",
    "    img_folder, label_folder, category_id, start_image_id, start_annotation_id = args\n",
    "    images = []\n",
    "    annotations = []\n",
    "    local_image_id = start_image_id\n",
    "    local_annotation_id = start_annotation_id\n",
    "\n",
    "    img_filenames = [f for f in os.listdir(img_folder) if f.endswith(\".jpg\")]\n",
    "\n",
    "    for filename in img_filenames:\n",
    "        image_path = os.path.join(img_folder, filename)\n",
    "        label_path = os.path.join(label_folder, filename.replace(\".jpg\", \".txt\"))\n",
    "\n",
    "        # Read image size\n",
    "        with Image.open(image_path) as img:\n",
    "            width, height = img.size\n",
    "\n",
    "        images.append({\n",
    "            \"id\": local_image_id,\n",
    "            \"file_name\": f\"{os.path.basename(img_folder)}/{filename}\",\n",
    "            \"width\": width,\n",
    "            \"height\": height\n",
    "        })\n",
    "\n",
    "        # Process annotation if exists\n",
    "        if os.path.isfile(label_path):\n",
    "            with open(label_path, \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "\n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 5:\n",
    "                    continue\n",
    "                cls, x_center, y_center, w, h = map(float, parts)\n",
    "\n",
    "                x = (x_center - w / 2) * width\n",
    "                y = (y_center - h / 2) * height\n",
    "                box_width = w * width\n",
    "                box_height = h * height\n",
    "\n",
    "                annotations.append({\n",
    "                    \"id\": local_annotation_id,\n",
    "                    \"image_id\": local_image_id,\n",
    "                    \"category_id\": category_id,\n",
    "                    \"bbox\": [x, y, box_width, box_height],\n",
    "                    \"area\": box_width * box_height,\n",
    "                    \"iscrowd\": 0\n",
    "                })\n",
    "                local_annotation_id += 1\n",
    "\n",
    "        local_image_id += 1\n",
    "\n",
    "    return images, annotations, local_image_id, local_annotation_id\n",
    "\n",
    "# Initialize\n",
    "coco_output = {\"images\": [], \"annotations\": [], \"categories\": []}  # Add your categories\n",
    "\n",
    "# Prepare stage paths\n",
    "stage_paths = [(os.path.join(image_dir, stage),\n",
    "                os.path.join(label_dir, stage),\n",
    "                category_mapping[stage],\n",
    "                0, 0)  # dummy start ids, will fix later\n",
    "               for stage in stage_folders]\n",
    "\n",
    "# Process stages in parallel\n",
    "all_results = []\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    all_results = list(executor.map(process_stage, stage_paths))\n",
    "\n",
    "# Merge results and fix IDs\n",
    "image_id = 0\n",
    "annotation_id = 0\n",
    "for images, annotations, _, _ in all_results:\n",
    "    id_mapping = {}\n",
    "    for img in images:\n",
    "        old_id = img[\"id\"]\n",
    "        img[\"id\"] = image_id\n",
    "        id_mapping[old_id] = image_id\n",
    "        coco_output[\"images\"].append(img)\n",
    "        image_id += 1\n",
    "\n",
    "    for anno in annotations:\n",
    "        anno[\"id\"] = annotation_id\n",
    "        anno[\"image_id\"] = id_mapping[anno[\"image_id\"]]  # fix image id\n",
    "        coco_output[\"annotations\"].append(anno)\n",
    "        annotation_id += 1\n",
    "\n",
    "# Save to JSON\n",
    "with open(coco_path, \"w\") as f:\n",
    "    json.dump(coco_output, f, indent=2)\n",
    "\n",
    "print(f\"COCO annotations saved to {coco_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train DINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial device check: cuda\n",
      "Initializing Image Processor...\n",
      "Preparing datasets...\n",
      "Train dataset size: 354196\n",
      "Eval dataset size: 39356\n",
      "Initializing DINOv2 Classifier model for fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Dinov2WithRegistersForImageClassification were not initialized from the model checkpoint at facebook/dinov2-with-registers-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='177104' max='177104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [177104/177104 46:57, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.107267</td>\n",
       "      <td>0.985898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100237</td>\n",
       "      <td>0.988032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.066600</td>\n",
       "      <td>0.093606</td>\n",
       "      <td>0.989201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to /workspace/dslab25/obj_detection/dino/dinov2_finetune/base/final_model\n",
      "Evaluating final model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2460' max='2460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2460/2460 01:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.09360600262880325, 'eval_accuracy': 0.9892011383270658, 'eval_runtime': 70.9364, 'eval_samples_per_second': 554.807, 'eval_steps_per_second': 34.679, 'epoch': 8.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pretrained_model = \"facebook/dinov2-with-registers-base\"\n",
    "\n",
    "# Determine device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Initial device check: {device}\")\n",
    "\n",
    "# Load COCO annotations and build the dataset dictionary\n",
    "with open(coco_path, 'r') as f:\n",
    "\tcoco_data = json.load(f)\n",
    "\n",
    "image_to_category = {ann[\"image_id\"]: ann[\"category_id\"] for ann in coco_data[\"annotations\"]}\n",
    "dataset_dict = {\"image_path\": [], \"label\": []}\n",
    "for image_info in coco_data[\"images\"]:\n",
    "\timage_id = image_info[\"id\"]\n",
    "\tfile_name = image_info[\"file_name\"]\n",
    "\tfull_path = os.path.join(image_dir, file_name)\n",
    "\tif image_id in image_to_category:\n",
    "\t\tdataset_dict[\"image_path\"].append(full_path)\n",
    "\t\tdataset_dict[\"label\"].append(image_to_category[image_id])\n",
    "\n",
    "# Split the dataset into train and validation (80/20)\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "print(\"Initializing Image Processor...\")\n",
    "processor = AutoImageProcessor.from_pretrained(pretrained_model)\n",
    "\n",
    "print(\"Preparing datasets...\")\n",
    "train_dataset = DINOv2Dataset(dataset[\"train\"], processor)\n",
    "eval_dataset = DINOv2Dataset(dataset[\"test\"], processor)\n",
    "\n",
    "print(\"Train dataset size:\", len(train_dataset))\n",
    "print(\"Eval dataset size:\", len(eval_dataset))\n",
    "\n",
    "print(\"Initializing DINOv2 Classifier model for fine-tuning...\")\n",
    "num_labels = len(set(dataset_dict[\"label\"]))\n",
    "model = DINOv2Classifier(num_labels=num_labels, pretrained_model=pretrained_model)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "\toutput_dir=os.path.join(dino_dir, \"dinov2_finetune/base\"),\n",
    "\tlearning_rate=1e-5,  # Lower learning rate for fine-tuning\n",
    "\tper_device_train_batch_size=16,  # Adjust batch size to your GPU memory\n",
    "\tper_device_eval_batch_size=16,\n",
    "\tnum_train_epochs=8,  # Fewer epochs may suffice for fine-tuning\n",
    "\tweight_decay=0.01,\n",
    "\teval_strategy=\"epoch\",\n",
    "\tsave_strategy=\"epoch\",\n",
    "\tload_best_model_at_end=True,\n",
    "\tdataloader_num_workers=4,\n",
    "\tlogging_steps=10,\n",
    "\tfp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "\tlogits, labels = eval_pred\n",
    "\tpredictions = np.argmax(logits, axis=1)\n",
    "\treturn metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "trainer = Trainer(\n",
    "\tmodel=model,\n",
    "\targs=training_args,\n",
    "\ttrain_dataset=train_dataset,\n",
    "\teval_dataset=eval_dataset,\n",
    "\tcompute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "\n",
    "model_save_path = os.path.join(training_args.output_dir, \"final_model\")\n",
    "trainer.save_model(model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "print(\"Evaluating final model...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Dino (on rendered images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of saved model\n",
    "model_dir = os.path.join(repo_dir, \"obj_detection/dinov2_finetune/base/final_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m full_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_dir, file_name)\n\u001b[1;32m     13\u001b[0m true_category_id \u001b[38;5;241m=\u001b[39m annotation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_path\u001b[49m\u001b[43m)\u001b[49m: \u001b[38;5;66;03m# Check if image file exists\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \tall_samples\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     16\u001b[0m \t\t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: full_path,\n\u001b[1;32m     17\u001b[0m \t\t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue_label_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: true_category_id\n\u001b[1;32m     18\u001b[0m \t})\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m<frozen genericpath>:19\u001b[0m, in \u001b[0;36mexists\u001b[0;34m(path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create mappings for easy lookup\n",
    "image_id_to_info = {img['id']: img for img in coco_data.get('images', [])}\n",
    "image_id_to_annotation = {ann['image_id']: ann for ann in coco_data.get('annotations', [])}\n",
    "category_id_to_name = {cat['id']: cat.get('name', f'category_{cat[\"id\"]}') for cat in coco_data.get('categories', [])}\n",
    "\n",
    "# --- Prepare list of all images with labels ---\n",
    "all_samples = [] \n",
    "for img_id, annotation in image_id_to_annotation.items():\n",
    "\tif img_id in image_id_to_info:\n",
    "\t\timage_info = image_id_to_info[img_id]\n",
    "\t\tfile_name = image_info['file_name']\n",
    "\t\tfull_path = os.path.join(image_dir, file_name)\n",
    "\t\ttrue_category_id = annotation['category_id']\n",
    "\t\tif os.path.exists(full_path): # Check if image file exists\n",
    "\t\t\tall_samples.append({\n",
    "\t\t\t\t\"image_path\": full_path,\n",
    "\t\t\t\t\"true_label_id\": true_category_id\n",
    "\t\t\t})\n",
    "\t\telse:\n",
    "\t\t\tprint(f\"Warning: Image file not found: {full_path}\")\n",
    "\telse:\n",
    "\t\tprint(f\"Warning: Image ID {img_id} found in annotations but not in images list.\")\n",
    "\n",
    "if not all_samples:\n",
    "\traise Exception(\"Error: No valid image samples found. Check image paths and COCO file.\")\n",
    "\n",
    "print(f\"Total valid samples found: {len(all_samples)}\")\n",
    "\n",
    "# --- Select 10 random samples ---\n",
    "num_samples_to_test = min(10, len(all_samples))\n",
    "if num_samples_to_test < 10:\n",
    "\tprint(f\"Warning: Fewer than 10 valid samples available. Testing on {num_samples_to_test}.\")\n",
    "random_samples = random.sample(all_samples, num_samples_to_test)\n",
    "\n",
    "# --- Load Model and Processor ---\n",
    "print(\"Loading image processor...\")\n",
    "processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
    "\n",
    "# Determine device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(f\"Loading model structure...\")\n",
    "# Assuming the number of labels can be inferred or is known (e.g., from categories in coco)\n",
    "num_labels = len(category_id_to_name) if category_id_to_name else 8 # Default to 8 if no categories\n",
    "if not category_id_to_name:\n",
    "\tprint(\"Warning: Could not determine number of labels from COCO categories. Defaulting to 8.\")\n",
    "\t\t\n",
    "model = DINOv2Classifier(num_labels=num_labels)\n",
    "\n",
    "# Construct the expected path to the weights file\n",
    "# Transformers Trainer saves as model.safetensors or pytorch_model.bin\n",
    "safetensors_path = os.path.join(model_dir, \"model.safetensors\")\n",
    "bin_path = os.path.join(model_dir, \"pytorch_model.bin\")\n",
    "\n",
    "model_weights_path = None\n",
    "if os.path.exists(safetensors_path):\n",
    "\tmodel_weights_path = safetensors_path\n",
    "elif os.path.exists(bin_path):\n",
    "\tmodel_weights_path = bin_path\n",
    "\t\n",
    "if model_weights_path:\n",
    "\tprint(f\"Loading model weights from: {model_weights_path}\")\n",
    "\ttry:\n",
    "\t\t# Use the correct loading function based on file type\n",
    "\t\tif model_weights_path.endswith(\".safetensors\"):\n",
    "\t\t\tstate_dict = load_safetensors(model_weights_path, device=str(device))\n",
    "\t\telse: # Assume .bin or other torch.load compatible format\n",
    "\t\t\t# Explicitly set weights_only=False for older formats if needed, True is safer default\n",
    "\t\t\tstate_dict = torch.load(model_weights_path, map_location=str(device), weights_only=True) \n",
    "\t\t\t\n",
    "\t\t# Adjust for potential DDP prefix ('module.') if saved during distributed training\n",
    "\t\tif next(iter(state_dict)).startswith('module.'):\n",
    "\t\t\tstate_dict = {k.partition('module.')[2]: v for k,v in state_dict.items()}\n",
    "\t\t\t\t\n",
    "\t\tmodel.load_state_dict(state_dict)\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"Error loading model weights: {e}\")\n",
    "\t\traise e\n",
    "else:\n",
    "\tprint(f\"Error: Model weights not found in {model_dir} (checked for model.safetensors and pytorch_model.bin)\")\n",
    "\traise Exception(\"Model weights not found\")\n",
    "\t\n",
    "\n",
    "model.to(device)\n",
    "model.eval() # Set model to evaluation mode\n",
    "print(\"\\n--- Starting Evaluation on Random Samples ---\")\n",
    "# --- Perform Inference ---\n",
    "correct_predictions = 0\n",
    "for i, sample in enumerate(random_samples):\n",
    "\timage_path = sample[\"image_path\"]\n",
    "\ttrue_label_id = sample[\"true_label_id\"]\n",
    "\ttrue_label_name = category_id_to_name.get(true_label_id, f\"ID_{true_label_id}\")\n",
    "\n",
    "\ttry:\n",
    "\t\timage = Image.open(image_path).convert(\"RGB\")\n",
    "\t\tdisplay(image)  # Display the image in Jupyter\n",
    "\n",
    "\t\tinputs = processor(images=image, return_tensors=\"pt\")\n",
    "\t\tpixel_values = inputs['pixel_values'].to(device)\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\toutputs = model(pixel_values=pixel_values)\n",
    "\n",
    "\t\tlogits = outputs['logits']\n",
    "\t\tpredicted_label_id = logits.argmax(-1).item()\n",
    "\t\tpredicted_label_name = category_id_to_name.get(predicted_label_id, f\"ID_{predicted_label_id}\")\n",
    "\n",
    "\t\tprint(f\"Sample {i+1}/{num_samples_to_test}:\")\n",
    "\t\tprint(f\"Image: {os.path.basename(image_path)}\")\n",
    "\t\tprint(f\"True Label: {true_label_name} (ID: {true_label_id})\")\n",
    "\t\tprint(f\"Predicted Label: {predicted_label_name} (ID: {predicted_label_id})\")\n",
    "\t\tif predicted_label_id == true_label_id:\n",
    "\t\t\tcorrect_predictions += 1\n",
    "\t\t\tprint(\"Result: ✅ CORRECT\")\n",
    "\t\telse:\n",
    "\t\t\tprint(\"Result: ❌ INCORRECT\")\n",
    "\t\tprint(\"-\" * 40)\n",
    "\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"Error processing sample {i+1} ({image_path}): {e}\")\n",
    "\t\tprint(\"-\" * 40)\n",
    "\n",
    "print(f\"\\nEvaluation Summary: {correct_predictions}/{num_samples_to_test} correct.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
