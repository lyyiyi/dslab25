{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from transformers import AutoImageProcessor\n",
    "from train import DINOv2Classifier\n",
    "from safetensors.torch import load_file as load_safetensors\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# CONFIGURATION\n",
    "repo_dir = os.getcwd().split('dslab25')[0] + 'dslab25/'\n",
    "video_path = os.path.join(repo_dir, \"assets/vacuum_pump/videos/01_run1_cam_2_1024x1024_15fps_3mbps.mp4\")\n",
    "labels_path = os.path.join(repo_dir, \"assets/vacuum_pump/videos/output.txt\")\n",
    "coco_path = os.path.join(repo_dir, \"training/vacuum_pump/coco_annotations.json\")\n",
    "# Path to your trained YOLOv12 weights (adjust as needed)\n",
    "yolo_model_path = os.path.join(repo_dir, \"obj_detection/dino/yolo_runs/yolov12_boundingbox\", \"weights\", \"best.pt\")\n",
    "temp_images_dir = os.path.join(repo_dir, \"temp_images\")\n",
    "anno_dir = os.path.join(repo_dir, \"assets/vacuum_pump/eval/original/class\")\n",
    "pretrained_model = \"facebook/dinov2-with-registers-large\"\n",
    "model_dir = os.path.join(repo_dir, \"obj_detection/dino/dinov2_finetune/checkpoint-14760/\")\n",
    "base_dir = os.path.join(repo_dir, \"training/vacuum_pump\")\n",
    "\n",
    "coco_path = os.path.join(base_dir, \"coco_annotations.json\")\n",
    "YOL_THRESHOLD = 0.38\n",
    "\n",
    "os.makedirs(temp_images_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labels from: /workspace/dslab25/assets/vacuum_pump/videos/output.txt\n",
      "Loading COCO annotations from: /workspace/dslab25/training/vacuum_pump/coco_annotations.json\n"
     ]
    }
   ],
   "source": [
    "def load_labels(labels_path):\n",
    "\t\"\"\"Load ground truth labels from file.\"\"\"\n",
    "\tframe_to_class = {}\n",
    "\twith open(labels_path, 'r') as f:\n",
    "\t\tfor line in f:\n",
    "\t\t\tparts = line.strip().split()\n",
    "\t\t\tif len(parts) == 3:\n",
    "\t\t\t\tstate_class, start_frame, end_frame = int(parts[0]), int(parts[1]), int(parts[2])\n",
    "\t\t\t\tfor frame_idx in range(start_frame, end_frame + 1):\n",
    "\t\t\t\t\tframe_to_class[frame_idx] = state_class\n",
    "\treturn frame_to_class\n",
    "\n",
    "# Load ground truth labels.\n",
    "print(f\"Loading labels from: {labels_path}\")\n",
    "frame_to_class = load_labels(labels_path)\n",
    "\n",
    "# Load COCO annotations to map category IDs to names.\n",
    "print(f\"Loading COCO annotations from: {coco_path}\")\n",
    "try:\n",
    "\twith open(coco_path, 'r') as f:\n",
    "\t\tcoco_data = json.load(f)\n",
    "\tcategory_id_to_name = {cat['id']: cat.get('name', f'category_{cat[\"id\"]}')\n",
    "\t\t\t\t\t\t\t\tfor cat in coco_data.get('categories', [])}\n",
    "except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "\tprint(f\"Error loading COCO annotations: {e}\")\n",
    "\tcategory_id_to_name = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLO model...\n",
      "Loading video from: /workspace/dslab25/assets/vacuum_pump/videos/01_run1_cam_2_1024x1024_15fps_3mbps.mp4\n",
      "Video info: 1856 frames, 15.0 fps\n",
      "\n",
      "--- Starting YOLO Evaluation ---\n",
      "Total frames to evaluate: 357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py:230: UserWarning: \n",
      "NVIDIA GeForce RTX 5090 with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_70 sm_75 sm_80 sm_86 sm_90.\n",
      "If you want to use the NVIDIA GeForce RTX 5090 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m frame_rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m     50\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(frame_rgb)\n\u001b[0;32m---> 52\u001b[0m yolo_results \u001b[38;5;241m=\u001b[39m \u001b[43myolo_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(yolo_results) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(yolo_results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mboxes) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     54\u001b[0m \t\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrame \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: No detection found. Skipping frame.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/model.py:182\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    155\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    156\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    158\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    159\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/model.py:543\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor \u001b[38;5;241m=\u001b[39m (predictor \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_smart_load(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictor\u001b[39m\u001b[38;5;124m\"\u001b[39m))(overrides\u001b[38;5;241m=\u001b[39margs, _callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks)\n\u001b[0;32m--> 543\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_cli\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# only update args if predictor is already setup\u001b[39;00m\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m get_cfg(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39margs, args)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/predictor.py:380\u001b[0m, in \u001b[0;36mBasePredictor.setup_model\u001b[0;34m(self, model, verbose)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    373\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    Initialize YOLO model with given parameters and set it to evaluation mode.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;124;03m        verbose (bool): Whether to print verbose output.\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 380\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mAutoBackend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mselect_device\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice  \u001b[38;5;66;03m# update device\u001b[39;00m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhalf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfp16  \u001b[38;5;66;03m# update half\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/nn/autobackend.py:164\u001b[0m, in \u001b[0;36mAutoBackend.__init__\u001b[0;34m(self, weights, device, dnn, data, fp16, batch, fuse, verbose)\u001b[0m\n\u001b[1;32m    162\u001b[0m model \u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fuse:\n\u001b[0;32m--> 164\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfuse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkpt_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    166\u001b[0m     kpt_shape \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mkpt_shape  \u001b[38;5;66;03m# pose-only\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/nn/tasks.py:211\u001b[0m, in \u001b[0;36mBaseModel.fuse\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, Conv2):\n\u001b[1;32m    210\u001b[0m     m\u001b[38;5;241m.\u001b[39mfuse_convs()\n\u001b[0;32m--> 211\u001b[0m m\u001b[38;5;241m.\u001b[39mconv \u001b[38;5;241m=\u001b[39m \u001b[43mfuse_conv_and_bn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# update conv\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28mdelattr\u001b[39m(m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbn\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# remove batchnorm\u001b[39;00m\n\u001b[1;32m    213\u001b[0m m\u001b[38;5;241m.\u001b[39mforward \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mforward_fuse  \u001b[38;5;66;03m# update forward\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/utils/torch_utils.py:259\u001b[0m, in \u001b[0;36mfuse_conv_and_bn\u001b[0;34m(conv, bn)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# Prepare filters\u001b[39;00m\n\u001b[1;32m    258\u001b[0m w_conv \u001b[38;5;241m=\u001b[39m conv\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mview(conv\u001b[38;5;241m.\u001b[39mout_channels, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 259\u001b[0m w_bn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiag(bn\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdiv(torch\u001b[38;5;241m.\u001b[39msqrt(\u001b[43mbn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m)))\n\u001b[1;32m    260\u001b[0m fusedconv\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mcopy_(torch\u001b[38;5;241m.\u001b[39mmm(w_bn, w_conv)\u001b[38;5;241m.\u001b[39mview(fusedconv\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mshape))\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# Prepare spatial bias\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Define label grouping logic\n",
    "def labels_match(pred, true):\n",
    "\tgroup_1 = {3, 4, 5}\n",
    "\tgroup_2 = {6, 7}\n",
    "\tif pred == true:\n",
    "\t\treturn True\n",
    "\tif pred in group_1 and true in group_1:\n",
    "\t\treturn True\n",
    "\tif pred in group_2 and true in group_2:\n",
    "\t\treturn True\n",
    "\treturn False\n",
    "\n",
    "print(\"Loading YOLO model...\")\n",
    "yolo_model = YOLO(yolo_model_path)\n",
    "\n",
    "try:\n",
    "\tfont = ImageFont.truetype(\"arial.ttf\", 16)\n",
    "except IOError:\n",
    "\tfont = ImageFont.load_default()\n",
    "\n",
    "print(f\"Loading video from: {video_path}\")\n",
    "video = cv2.VideoCapture(video_path)\n",
    "if not video.isOpened():\n",
    "\traise Exception(\"Error: Could not open video file.\")\n",
    "\t\n",
    "total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "fps = video.get(cv2.CAP_PROP_FPS)\n",
    "print(f\"Video info: {total_frames} frames, {fps} fps\")\n",
    "\n",
    "print(\"\\n--- Starting YOLO Evaluation ---\")\n",
    "frame_idx = 0\n",
    "frames_to_process = []\n",
    "while True:\n",
    "\tret, frame = video.read()\n",
    "\tif not ret:\n",
    "\t\tbreak\n",
    "\tif frame_idx % 5 == 0 and frame_idx in frame_to_class:\n",
    "\t\tframes_to_process.append((frame_idx, frame))\n",
    "\tframe_idx += 1\n",
    "video.release()\n",
    "print(f\"Total frames to evaluate: {len(frames_to_process)}\")\n",
    "\n",
    "os.makedirs(temp_images_dir, exist_ok=True)\n",
    "\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "for frame_idx, frame in frames_to_process:\n",
    "\tframe_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\timage = Image.fromarray(frame_rgb)\n",
    "\n",
    "\tyolo_results = yolo_model(frame)\n",
    "\tif len(yolo_results) == 0 or len(yolo_results[0].boxes) == 0:\n",
    "\t\tprint(f\"Frame {frame_idx}: No detection found. Skipping frame.\")\n",
    "\t\tcontinue\n",
    "\n",
    "\tboxes = yolo_results[0].boxes.data  # Each row: [x1, y1, x2, y2, conf, cls]\n",
    "\ttrue_label = frame_to_class[frame_idx]\n",
    "\ttrue_label_name = category_id_to_name.get(true_label, f\"Class_{true_label}\")\n",
    "\n",
    "\tfor idx, box in enumerate(boxes):\n",
    "\t\tconfidence = box[4].item()\n",
    "\t\tif confidence < YOL_THRESHOLD:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tpredicted_label = int(box[5].item())\n",
    "\t\tpredicted_label_name = category_id_to_name.get(predicted_label, f\"Class_{predicted_label}\")\n",
    "\n",
    "\t\tx1, y1, x2, y2 = map(int, box[:4].tolist())\n",
    "\t\tx1 = max(0, x1)\n",
    "\t\ty1 = max(0, y1)\n",
    "\t\tx2 = min(image.width, x2)\n",
    "\t\ty2 = min(image.height, y2)\n",
    "\n",
    "\t\tcropped_image = image.crop((x1, y1, x2, y2))\n",
    "\t\tcropped_path = os.path.join(temp_images_dir, f\"frame_{frame_idx}.jpg\")\n",
    "\t\tcropped_image.save(cropped_path)\n",
    "\n",
    "\t\tprint(f\"Saved cropped image: {cropped_path}\")\n",
    "\n",
    "\t\tis_correct = labels_match(predicted_label, true_label)\n",
    "\t\tif is_correct:\n",
    "\t\t\tcorrect_predictions += 1\n",
    "\t\ttotal_predictions += 1\n",
    "\n",
    "\t\tprint(f\"Frame {frame_idx} | Box {idx}\")\n",
    "\t\tprint(f\"  True:\t  {true_label_name} (ID: {true_label})\")\n",
    "\t\tprint(f\"  Predicted: {predicted_label_name} (ID: {predicted_label}) | Conf: {confidence:.2f}\")\n",
    "\t\tprint(f\"  Correct:   {'✅ Yes' if is_correct else '❌ No'}\")\n",
    "\t\tprint(\"-\" * 30)\n",
    "\n",
    "accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "print(\"\\nEvaluation Summary:\")\n",
    "print(f\"  Total predictions: {total_predictions}\")\n",
    "print(f\"  Correct predictions: {correct_predictions}\")\n",
    "print(f\"  Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DinoV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading image processor...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25c6e7256ee4e5191dd86352426fafa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/436 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Number of classes: 8\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b8054db44294af68c4789a23e1cfe1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51964b2427864e3db795f1c224915deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Dinov2WithRegistersForImageClassification were not initialized from the model checkpoint at facebook/dinov2-with-registers-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model weights from: /workspace/dslab25/obj_detection/dino/dinov2_finetune/checkpoint-14760/model.safetensors\n",
      "\n",
      "Evaluation Summary:\n",
      "  Total evaluated: 0\n",
      "  Correct:\t\t 0\n",
      "  Accuracy:\t\t0.00\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "image_files = sorted(glob(os.path.join(temp_images_dir, \"*.jpg\")))\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Load image processor and model\n",
    "print(\"Loading image processor...\")\n",
    "processor = AutoImageProcessor.from_pretrained(pretrained_model)\n",
    "\n",
    "# Determine device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Get number of classes from frame_to_class\n",
    "num_labels = max(frame_to_class.values()) + 1 if frame_to_class else 8\n",
    "print(f\"Number of classes: {num_labels}\")\n",
    "\n",
    "# Load model\n",
    "print(\"Loading model...\")\n",
    "model = DINOv2Classifier(num_labels=num_labels, pretrained_model=pretrained_model)\n",
    "\n",
    "# Load model weights\n",
    "safetensors_path = os.path.join(model_dir, \"model.safetensors\")\n",
    "bin_path = os.path.join(model_dir, \"pytorch_model.bin\")\n",
    "\n",
    "model_weights_path = None\n",
    "if os.path.exists(safetensors_path):\n",
    "\tmodel_weights_path = safetensors_path\n",
    "elif os.path.exists(bin_path):\n",
    "\tmodel_weights_path = bin_path\n",
    "\t\n",
    "if model_weights_path:\n",
    "\tprint(f\"Loading model weights from: {model_weights_path}\")\n",
    "\ttry:\n",
    "\t\tif model_weights_path.endswith(\".safetensors\"):\n",
    "\t\t\tstate_dict = load_safetensors(model_weights_path, device=str(device))\n",
    "\t\telse:\n",
    "\t\t\tstate_dict = torch.load(model_weights_path, map_location=str(device), weights_only=True)\n",
    "\t\t\t\n",
    "\t\t# Handle potential DDP prefix\n",
    "\t\tif next(iter(state_dict)).startswith('module.'):\n",
    "\t\t\tstate_dict = {k.partition('module.')[2]: v for k,v in state_dict.items()}\n",
    "\t\t\t\t\n",
    "\t\tmodel.load_state_dict(state_dict)\n",
    "\texcept Exception as e:\n",
    "\t\traise e\n",
    "else:\n",
    "\traise Exception(f\"Error: Model weights not found in {model_dir}\")\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for image_path in image_files:\n",
    "\t# Infer annotation file path\n",
    "\tfilename = os.path.splitext(os.path.basename(image_path))[0]\n",
    "\tanno_path = os.path.join(anno_dir, f\"{filename}.txt\")\n",
    "\t\n",
    "\tif not os.path.exists(anno_path):\n",
    "\t\tprint(f\"Annotation not found for {filename}, skipping.\")\n",
    "\t\tcontinue\n",
    "\n",
    "\t# Read true class label\n",
    "\twith open(anno_path, \"r\") as f:\n",
    "\t\ttry:\n",
    "\t\t\ttrue_label = int(f.readline().strip())\n",
    "\t\texcept ValueError:\n",
    "\t\t\tprint(f\"Invalid label in {anno_path}, skipping.\")\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t# Load image\n",
    "\timage = Image.open(image_path).convert(\"RGB\")\n",
    "\tdisplay(image)\n",
    "\n",
    "\t# Process image\n",
    "\tinputs = processor(images=image, return_tensors=\"pt\")\n",
    "\tpixel_values = inputs[\"pixel_values\"].to(device)\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\toutputs = model(pixel_values=pixel_values)\n",
    "\t\n",
    "\tlogits = outputs[\"logits\"]\n",
    "\tpredicted_label = logits.argmax(-1).item()\n",
    "\n",
    "\tdef labels_match(pred, true):\n",
    "\t\tgroup_1 = {3, 4, 5}\n",
    "\t\tgroup_2 = {6, 7}\n",
    "\t\tif pred == true:\n",
    "\t\t\treturn True\n",
    "\t\tif pred in group_1 and true in group_1:\n",
    "\t\t\treturn True\n",
    "\t\tif pred in group_2 and true in group_2:\n",
    "\t\t\treturn True\n",
    "\t\treturn False\n",
    "\n",
    "\t# Inside your loop:\n",
    "\tis_correct = labels_match(predicted_label, true_label)\n",
    "\tif is_correct:\n",
    "\t\tcorrect_predictions += 1\n",
    "\ttotal_predictions += 1\n",
    "\n",
    "\tprint(f\"{filename}:\")\n",
    "\tprint(f\"  True:\t{true_label}\")\n",
    "\tprint(f\"  Predicted: {predicted_label}\")\n",
    "\tprint(f\"  Correct:   {'✅ Yes' if is_correct else '❌ No'}\")\n",
    "\tprint(\"-\" * 30)\n",
    "\n",
    "accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "print(\"\\nEvaluation Summary:\")\n",
    "print(f\"  Total evaluated: {total_predictions}\")\n",
    "print(f\"  Correct:\t\t {correct_predictions}\")\n",
    "print(f\"  Accuracy:\t\t{accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen VL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcd22dc8c1cb4973b1244b4f2167ef87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7597db8a4f947a1986a2de51a19b29d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/109k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c401cf43ba4078b5a75bb7ad5c4b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 38 files:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ff52bc5f9c04883b3b9fa66fccb4917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f811752c73e64c1b9bcaa8c6ba21de5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb5ac6baf59a4f37ac1014997088ea8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "229978c595f04f12950e8622eb91e42c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00038.safetensors:   0%|          | 0.00/3.81G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "496b1ab0cc304bb4879a058f1e6da162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00038.safetensors:   0%|          | 0.00/3.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0b935e308d42da83717451847483c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa04d93f85b4c5a9106341052641eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00038.safetensors:   0%|          | 0.00/3.81G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4369ff4c653a48d8bee9b3ff676f0345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00038.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/contrib/concurrent.py:51\u001b[0m, in \u001b[0;36m_executor_map\u001b[0;34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m PoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_workers, initializer\u001b[38;5;241m=\u001b[39mtqdm_class\u001b[38;5;241m.\u001b[39mset_lock,\n\u001b[1;32m     50\u001b[0m                   initargs\u001b[38;5;241m=\u001b[39m(lk,)) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/usr/lib/python3.11/threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m     \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m\n\u001b[1;32m      4\u001b[0m states \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m \t\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_0\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBase block metal piece\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m \t\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCylinder metal piece which gets stick on the base block stage_0\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \t\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_7\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5 screws now get screwed onto the piece\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     13\u001b[0m }\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m qwen_vl_model \u001b[38;5;241m=\u001b[39m \u001b[43mQwen2_5_VLForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m\t\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQwen/Qwen2.5-VL-72B-Instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflash_attention_2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# default processer\u001b[39;00m\n\u001b[1;32m     24\u001b[0m qwen_vl_processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen2.5-VL-72B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:279\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:4261\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4252\u001b[0m     gguf_file\n\u001b[1;32m   4253\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4254\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[1;32m   4255\u001b[0m ):\n\u001b[1;32m   4256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   4257\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4258\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded from GGUF files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4259\u001b[0m     )\n\u001b[0;32m-> 4261\u001b[0m checkpoint_files, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4263\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4268\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4274\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4277\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4279\u001b[0m is_sharded \u001b[38;5;241m=\u001b[39m sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4280\u001b[0m is_quantized \u001b[38;5;241m=\u001b[39m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:1152\u001b[0m, in \u001b[0;36m_get_resolved_checkpoint_files\u001b[0;34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash)\u001b[0m\n\u001b[1;32m   1150\u001b[0m sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[0;32m-> 1152\u001b[0m     checkpoint_files, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_checkpoint_shard_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1166\u001b[0m     checkpoint_files \u001b[38;5;241m=\u001b[39m [resolved_archive_file] \u001b[38;5;28;01mif\u001b[39;00m pretrained_model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py:1115\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m shard_filenames, sharded_metadata\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;66;03m# At this stage pretrained_model_name_or_path is a model identifier on the Hub. Try to get everything from cache,\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;66;03m# or download the files\u001b[39;00m\n\u001b[0;32m-> 1115\u001b[0m cached_filenames \u001b[38;5;241m=\u001b[39m \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshard_filenames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached_filenames, sharded_metadata\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py:439\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m         hf_hub_download(\n\u001b[1;32m    425\u001b[0m             path_or_repo_id,\n\u001b[1;32m    426\u001b[0m             filenames[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    436\u001b[0m             local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    437\u001b[0m         )\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m         \u001b[43msnapshot_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_patterns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_filenames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;66;03m# We cannot recover from them\u001b[39;00m\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RepositoryNotFoundError) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, GatedRepoError):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/_snapshot_download.py:296\u001b[0m, in \u001b[0;36msnapshot_download\u001b[0;34m(repo_id, repo_type, revision, cache_dir, local_dir, library_name, library_version, user_agent, proxies, etag_timeout, force_download, token, local_files_only, allow_patterns, ignore_patterns, max_workers, tqdm_class, headers, endpoint, local_dir_use_symlinks, resume_download)\u001b[0m\n\u001b[1;32m    294\u001b[0m         _inner_hf_hub_download(file)\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 296\u001b[0m     \u001b[43mthread_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_inner_hf_hub_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiltered_repo_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFetching \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfiltered_repo_files\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m files\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# User can use its own tqdm class or the default one from `huggingface_hub.utils`\u001b[39;49;00m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtqdm_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhf_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(local_dir))\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/contrib/concurrent.py:69\u001b[0m, in \u001b[0;36mthread_map\u001b[0;34m(fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03mEquivalent of `list(map(fn, *iterables))`\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03mdriven by `concurrent.futures.ThreadPoolExecutor`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    [default: max(32, cpu_count() + 4)].\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfutures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_executor_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtqdm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/contrib/concurrent.py:49\u001b[0m, in \u001b[0;36m_executor_map\u001b[0;34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m lock_name \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlock_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ensure_lock(tqdm_class, lock_name\u001b[38;5;241m=\u001b[39mlock_name) \u001b[38;5;28;01mas\u001b[39;00m lk:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# share lock in case workers are already using `tqdm`\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mPoolExecutor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtqdm_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_lock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mex\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py:647\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 647\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/concurrent/futures/thread.py:235\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads:\n\u001b[0;32m--> 235\u001b[0m         \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/threading.py:1119\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1119\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/usr/lib/python3.11/threading.py:1139\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1140\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "states = {\n",
    "\t'state_0': 'Base block metal piece',\n",
    "\t'state_1': 'Cylinder metal piece which gets stick on the base block stage_0',\n",
    "\t'state_2': 'A Big metal piece which gets stick on the cylinder piece of stage_1',\n",
    "\t'state_3': 'A smaller thin metal piece which gets put onto the center of the big metal piece of stage_2',\n",
    "\t'state_4': 'A tiny metal ring which gets placed onto the center of the thing metal piece of stage_3',\n",
    "\t'state_5': '3 screws now get screwed onto the piece',\n",
    "\t'state_6': 'A darker metal plate now gets placed on top of the piece',\n",
    "\t'state_7': '5 screws now get screwed onto the piece',\n",
    "}\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "qwen_vl_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "\t\"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "\ttorch_dtype=torch.bfloat16,\n",
    "\tattn_implementation=\"flash_attention_2\",\n",
    "\tdevice_map=\"auto\",\n",
    ")\n",
    "\n",
    "# default processer\n",
    "qwen_vl_processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "\n",
    "# Messages containing a local video path and a text query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Messages containing a local video path and a text query\n",
    "\n",
    "def qwen_vl(image_paths: list[str], current_dino_state: str, previous_qwen_state: str, previous_information: str):\n",
    "\t\"\"\"\n",
    "\t[\n",
    "\t\t\"file:///path/to/frame1.jpg\",\n",
    "\t\t\"file:///path/to/frame2.jpg\",\n",
    "\t\t\"file:///path/to/frame3.jpg\",\n",
    "\t\t\"file:///path/to/frame4.jpg\",\n",
    "\t],\n",
    "\t\"\"\"\n",
    "\tmessages = [\n",
    "\t\t{\n",
    "\t\t\t\"role\": \"user\",\n",
    "\t\t\t\"content\": [\n",
    "\t\t\t{\n",
    "\t\t\t\t\"type\": \"video\",\n",
    "\t\t\t\t\"video\": image_paths\n",
    "\t\t\t},\n",
    "\t\t\t{\n",
    "\t\t\t\t\"type\": \"text\",\n",
    "\t\t\t\t\"text\": f'''\n",
    "\t\t\t\t\tYou are given a short video clip of a person assembling an object. The clip is part of a longer clip of the person assembling the object.\n",
    "\t\t\t\t\tYour task is it to tell me in which state the piece is at the end of the clip. Note that the clip may end while the person is still in the middle of an action.\n",
    "\t\t\t\t\tIf the state is in between two states, return the state it was in before, so if the person is moving the cylinder piece to the base block, return state_0, only when the cylinder piece is on the base block, return state_1.\n",
    "\t\t\t\t\tHere are the possible states:\n",
    "\t\t\t\t\t\t'state_0': 'First part of the object: Base block metal piece',\n",
    "\t\t\t\t\t\t'state_1': 'Second part of the object: Cylinder metal piece which gets stick on the base block stage_0',\n",
    "\t\t\t\t\t\t'state_2': 'Third part of the object: A Big metal piece which gets stick on the cylinder piece of stage_1',\n",
    "\t\t\t\t\t\t'state_3': 'Fourth part of the object: A smaller thin metal piece which gets put onto the center of the big metal piece of stage_2',\n",
    "\t\t\t\t\t\t'state_4': 'Fifth part of the object: A tiny metal ring which gets placed onto the center of the thing metal piece of stage_3',\n",
    "\t\t\t\t\t\t'state_5': 'Sixth part of the object: 3 screws now get screwed onto the piece',\n",
    "\t\t\t\t\t\t'state_6': 'Seventh part of the object: A darker metal plate now gets placed on top of the piece',\n",
    "\t\t\t\t\t\t'state_7': 'Eighth part of the object: 5 screws now get screwed onto the piece'\n",
    "\t\t\t\t\tAdditionally I will give you the state in which the classifier thinks the piece is at.\n",
    "\t\t\t\t\tHere is the information:\n",
    "\t\t\t\t\t\t{current_dino_state}\n",
    "\t\t\t\t\tAdditionally I will give you the state in which the classifier thinks the piece was at at the beginning of the clip.\n",
    "\t\t\t\t\tHere is the information:\n",
    "\t\t\t\t\t\t{previous_qwen_state}\n",
    "\t\t\t\t\tAdditionally, I will give you some information about what the person did 5 seconds before the clip starts.\n",
    "\t\t\t\t\tHere is the information:\n",
    "\t\t\t\t\t\t{previous_information}\n",
    "\n",
    "\t\t\t\t\tPlease return the output in the following format:\n",
    "\t\t\t\t\t{\n",
    "\t\t\t\t\t\t\"state\": \"state_name\",\n",
    "\t\t\t\t\t\t\"action_description\": \"action_description\"\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\t'''},\n",
    "\t\t\t\t],\n",
    "\t\t\t}\n",
    "\t\t]\n",
    "\n",
    "\ttext = qwen_vl_processor.apply_chat_template(\n",
    "\t\tmessages, tokenize=False, add_generation_prompt=True\n",
    "\t)\n",
    "\timage_inputs, video_inputs, video_kwargs = process_vision_info(messages, return_video_kwargs=True)\n",
    "\tinputs = qwen_vl_processor(\n",
    "\t\ttext=[text],\n",
    "\t\timages=image_inputs,\n",
    "\t\tvideos=video_inputs,\n",
    "\t\tfps=fps,\n",
    "\t\tpadding=True,\n",
    "\t\treturn_tensors=\"pt\",\n",
    "\t\t**video_kwargs,\n",
    "\t)\n",
    "\tinputs = inputs.to(\"cuda\")\n",
    "\n",
    "\t# Inference\n",
    "\tgenerated_ids = qwen_vl_model.generate(**inputs, max_new_tokens=128)\n",
    "\tgenerated_ids_trimmed = [\n",
    "\t\tout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "\t]\n",
    "\toutput_text = qwen_vl_processor.batch_decode(\n",
    "\t\tgenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "\t)\n",
    "\tprint(output_text)\n",
    "\treturn output_text\n",
    "\n",
    "previous_qwen_state = \"None because its the first clip\"\n",
    "previous_information = \"None because its the first clip\"\n",
    "current_dino_state = \"None because its the first clip\"\n",
    "\n",
    "for idx in range(0, len(frames_to_process), 9):\n",
    "\timage_paths = []\n",
    "\tfor frame_idx, frame in frames_to_process[idx:idx+9]:\n",
    "\t\tframe_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\t\timage = Image.fromarray(frame_rgb)\n",
    "\t\timage_path = os.path.join(temp_images_dir, f\"frame_{frame_idx}_original.jpg\")\n",
    "\t\timage.save(image_path)\n",
    "\t\timage_paths.append(image_path)\n",
    "\n",
    "\tyolo_results = yolo_model(frame)\n",
    "\tif len(yolo_results) == 0 or len(yolo_results[0].boxes) == 0:\n",
    "\t\tprint(f\"Frame {frame_idx}: No detection found. Skipping frame.\")\n",
    "\t\tcontinue\n",
    "\n",
    "\tboxes = yolo_results[0].boxes.data  # Each row: [x1, y1, x2, y2, conf, cls]\n",
    "\ttrue_label = frame_to_class[frame_idx]\n",
    "\ttrue_label_name = category_id_to_name.get(true_label, f\"CYlass_{true_label}\")\n",
    "\t\n",
    "\t# Find the box with the highest confidence\n",
    "\thighest_conf_idx = -1\n",
    "\thighest_conf = 0\n",
    "\t\n",
    "\tfor idx, box in enumerate(boxes):\n",
    "\t\tconfidence = box[4].item()\n",
    "\t\tif confidence < YOL_THRESHOLD:\n",
    "\t\t\tcontinue\n",
    "\t\t\n",
    "\t\tif confidence > highest_conf:\n",
    "\t\t\thighest_conf = confidence\n",
    "\t\t\thighest_conf_idx = idx\n",
    "\t\n",
    "\tif highest_conf_idx >= 0:\n",
    "\t\tbox = boxes[highest_conf_idx]\n",
    "\t\tconfidence = box[4].item()\n",
    "\t\tpredicted_label = int(box[5].item())\n",
    "\t\tcurrent_dino_state = category_id_to_name.get(predicted_label, f\"Class_{predicted_label}\")\n",
    "\t\t\n",
    "\t\tx1, y1, x2, y2 = map(int, box[:4].tolist())\n",
    "\t\tx1 = max(0, x1)\n",
    "\t\ty1 = max(0, y1)\n",
    "\t\tx2 = min(image.width, x2)\n",
    "\t\ty2 = min(image.height, y2)\n",
    "\t\t\n",
    "\t\tcropped_image = image.crop((x1, y1, x2, y2))\n",
    "\t\tcropped_path = os.path.join(temp_images_dir, f\"frame_{frame_idx}.jpg\")\n",
    "\t\tcropped_image.save(cropped_path)\n",
    "\t\t\n",
    "\t\tprint(f\"Saved cropped image: {cropped_path}\")\n",
    "\telse:\n",
    "\t\tcurrent_dino_state = \"None, Dino didnt make a prediction\"\n",
    "\tres = qwen_vl(image_paths, current_dino_state, previous_qwen_state, previous_information)\n",
    "\tres = json.loads(res)\n",
    "\n",
    "\tprevious_qwen_state = res[\"state\"]\n",
    "\tprevious_information = res[\"action_description\"]\n",
    "\n",
    "\tis_correct = labels_match(previous_qwen_state, true_label)\n",
    "\tif is_correct:\n",
    "\t\tcorrect_predictions += 1\n",
    "\ttotal_predictions += 1\n",
    "\t\n",
    "\tdisplay(image)\n",
    "\tprint(f\"Frame {frame_idx}\")\n",
    "\tprint(f\"  True:\t{true_label}\")\n",
    "\tprint(f\"  Predicted: {predicted_label}\")\n",
    "\tprint(f\"  Correct:   {'✅ Yes' if is_correct else '❌ No'}\")\n",
    "\tprint(\"-\" * 30)\n",
    "\n",
    "\n",
    "\n",
    "accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "print(\"\\nEvaluation Summary:\")\n",
    "print(f\"  Total predictions: {total_predictions}\")\n",
    "print(f\"  Correct predictions: {correct_predictions}\")\n",
    "print(f\"  Accuracy: {accuracy:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
