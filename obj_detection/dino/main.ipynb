{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from datasets import Dataset\n",
    "from transformers import AutoImageProcessor, Trainer, TrainingArguments\n",
    "import evaluate\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO annotations saved to /Users/georgye/Documents/repos/ethz/dslab25/training/vacuum_pump/coco_annotations.json\n"
     ]
    }
   ],
   "source": [
    "# CONFIG\n",
    "base_dir = \"/Users/georgye/Documents/repos/ethz/dslab25/\"\n",
    "root_dir = base_dir + \"training/vacuum_pump\"\n",
    "image_dir = os.path.join(root_dir, \"images\", \"augmented\")\n",
    "label_dir = os.path.join(root_dir, \"annotation\", \"augmented\")\n",
    "coco_path = os.path.join(root_dir, \"coco_annotations.json\")\n",
    "\n",
    "# Collect category mapping\n",
    "stage_folders = [f\"stage_{i}\" for i in range(8)]\n",
    "category_mapping = {name: i for i, name in enumerate(stage_folders)}  # name -> ID\n",
    "categories = [{\"id\": i, \"name\": name} for name, i in category_mapping.items()]\n",
    "\n",
    "# Initialize COCO structure\n",
    "coco_output = {\n",
    "\t\"images\": [],\n",
    "\t\"annotations\": [],\n",
    "\t\"categories\": categories\n",
    "}\n",
    "\n",
    "image_id = 0\n",
    "annotation_id = 0\n",
    "\n",
    "# Traverse through each stage folder\n",
    "for class_folder in stage_folders:\n",
    "\timg_folder = os.path.join(image_dir, class_folder)\n",
    "\tlabel_folder = os.path.join(label_dir, class_folder)\n",
    "\tcategory_id = category_mapping[class_folder]\n",
    "\n",
    "\tfor filename in os.listdir(img_folder):\n",
    "\t\tif not filename.endswith(\".jpg\"):\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\timage_path = os.path.join(img_folder, filename)\n",
    "\t\tlabel_path = os.path.join(label_folder, filename.replace(\".jpg\", \".txt\"))\n",
    "\n",
    "\t\t# Read image size\n",
    "\t\twith Image.open(image_path) as img:\n",
    "\t\t\twidth, height = img.size\n",
    "\n",
    "\t\t# Add image entry\n",
    "\t\tcoco_output[\"images\"].append({\n",
    "\t\t\t\"id\": image_id,\n",
    "\t\t\t\"file_name\": f\"{class_folder}/{filename}\",\n",
    "\t\t\t\"width\": width,\n",
    "\t\t\t\"height\": height\n",
    "\t\t})\n",
    "\n",
    "\t\t# Process annotation\n",
    "\t\tif os.path.exists(label_path):\n",
    "\t\t\twith open(label_path, \"r\") as f:\n",
    "\t\t\t\tfor line in f:\n",
    "\t\t\t\t\tparts = line.strip().split()\n",
    "\t\t\t\t\tif len(parts) != 5:\n",
    "\t\t\t\t\t\tcontinue\n",
    "\t\t\t\t\tcls, x_center, y_center, w, h = map(float, parts)\n",
    "\n",
    "\t\t\t\t\t# Convert YOLO to COCO format\n",
    "\t\t\t\t\tx = (x_center - w / 2) * width\n",
    "\t\t\t\t\ty = (y_center - h / 2) * height\n",
    "\t\t\t\t\tbox_width = w * width\n",
    "\t\t\t\t\tbox_height = h * height\n",
    "\n",
    "\t\t\t\t\tcoco_output[\"annotations\"].append({\n",
    "\t\t\t\t\t\t\"id\": annotation_id,\n",
    "\t\t\t\t\t\t\"image_id\": image_id,\n",
    "\t\t\t\t\t\t\"category_id\": category_id,\n",
    "\t\t\t\t\t\t\"bbox\": [x, y, box_width, box_height],\n",
    "\t\t\t\t\t\t\"area\": box_width * box_height,\n",
    "\t\t\t\t\t\t\"iscrowd\": 0\n",
    "\t\t\t\t\t})\n",
    "\t\t\t\t\tannotation_id += 1\n",
    "\n",
    "\t\timage_id += 1\n",
    "\n",
    "# Save to JSON\n",
    "with open(coco_path, \"w\") as f:\n",
    "\tjson.dump(coco_output, f, indent=2)\n",
    "\n",
    "print(f\"COCO annotations saved to {coco_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DINOv2 ViT-B/14 with registers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/georgye/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Preparing datasets...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "--load_best_model_at_end requires the save and eval strategy to match, but found\n- Evaluation strategy: IntervalStrategy.NO\n- Save strategy: SaveStrategy.EPOCH",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 91\u001b[39m\n\u001b[32m     88\u001b[39m model = LinearClassifier()\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# Define training arguments\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdinov2_register_linear_classifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Can increase this on RTX 4090\u001b[39;49;00m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataloader_num_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use multiple workers for data loading\u001b[39;49;00m\n\u001b[32m    101\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# Define compute_metrics function\u001b[39;00m\n\u001b[32m    104\u001b[39m metric = evaluate.load(\u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:132\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, tp_size, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, average_tokens_across_devices)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ethz/dslab25/venv/dslab_py311/lib/python3.11/site-packages/transformers/training_args.py:1648\u001b[39m, in \u001b[36mTrainingArguments.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1646\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.load_best_model_at_end \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save_strategy != SaveStrategy.BEST:\n\u001b[32m   1647\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.eval_strategy != \u001b[38;5;28mself\u001b[39m.save_strategy:\n\u001b[32m-> \u001b[39m\u001b[32m1648\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1649\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m--load_best_model_at_end requires the save and eval strategy to match, but found\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m- Evaluation \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1650\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mstrategy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.eval_strategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m- Save strategy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.save_strategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1651\u001b[39m         )\n\u001b[32m   1652\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.eval_strategy == IntervalStrategy.STEPS \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save_steps % \u001b[38;5;28mself\u001b[39m.eval_steps != \u001b[32m0\u001b[39m:\n\u001b[32m   1653\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.eval_steps < \u001b[32m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save_steps < \u001b[32m1\u001b[39m:\n",
      "\u001b[31mValueError\u001b[39m: --load_best_model_at_end requires the save and eval strategy to match, but found\n- Evaluation strategy: IntervalStrategy.NO\n- Save strategy: SaveStrategy.EPOCH"
     ]
    }
   ],
   "source": [
    "# Load COCO annotations\n",
    "with open(coco_path, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Create image-to-category mapping\n",
    "image_to_category = {}\n",
    "for annotation in coco_data[\"annotations\"]:\n",
    "    image_id = annotation[\"image_id\"]\n",
    "    category_id = annotation[\"category_id\"]\n",
    "    image_to_category[image_id] = category_id\n",
    "\n",
    "# Create dataset dictionaries\n",
    "dataset_dict = {\n",
    "    \"image_path\": [],\n",
    "    \"label\": []\n",
    "}\n",
    "\n",
    "for image_info in coco_data[\"images\"]:\n",
    "    image_id = image_info[\"id\"]\n",
    "    file_name = image_info[\"file_name\"]\n",
    "    full_path = os.path.join(image_dir, file_name)\n",
    "    \n",
    "    if image_id in image_to_category:\n",
    "        dataset_dict[\"image_path\"].append(full_path)\n",
    "        dataset_dict[\"label\"].append(image_to_category[image_id])\n",
    "\n",
    "# Split dataset into train and validation (80/20)\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Load DINOv2 model with registers via torch hub\n",
    "# Using ViT-B/14 with registers which has improved performance\n",
    "print(\"Loading DINOv2 ViT-B/14 with registers...\")\n",
    "dinov2 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14_reg')\n",
    "dinov2.eval()  # Set to evaluation mode\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dinov2 = dinov2.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define image processor\n",
    "processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
    "\n",
    "# Custom Dataset for DINOv2 features\n",
    "class DINOv2Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_dict, processor, feature_extractor, device):\n",
    "        self.dataset = dataset_dict\n",
    "        self.processor = processor\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.device = device\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = Image.open(item[\"image_path\"]).convert(\"RGB\")\n",
    "        \n",
    "        # Process image for DINOv2\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Extract features - use the class token ([CLS])\n",
    "        with torch.no_grad():\n",
    "            features = self.feature_extractor(**inputs).last_hidden_state[:, 0].squeeze().cpu()\n",
    "        \n",
    "        return {\n",
    "            \"features\": features,\n",
    "            \"labels\": torch.tensor(item[\"label\"], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Prepare datasets\n",
    "print(\"Preparing datasets...\")\n",
    "train_dataset = DINOv2Dataset(dataset[\"train\"], processor, dinov2, device)\n",
    "eval_dataset = DINOv2Dataset(dataset[\"test\"], processor, dinov2, device)\n",
    "\n",
    "# Define linear classifier\n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=768, num_labels=8):  # 768 is ViT-B dimension\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Linear(input_dim, num_labels)\n",
    "        \n",
    "    def forward(self, features):\n",
    "        return {\"logits\": self.classifier(features)}\n",
    "\n",
    "# Initialize model\n",
    "model = LinearClassifier()\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(root_dir, \"dinov2_register_linear_classifier\"),\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=64,  # Can increase this on RTX 4090\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    dataloader_num_workers=4,  # Use multiple workers for data loading\n",
    ")\n",
    "\n",
    "# Define compute_metrics function\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model_save_path = os.path.join(root_dir, \"dinov2_register_linear_classifier_final\")\n",
    "trainer.save_model(model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")\n",
    "\n",
    "# Example of how to use the model for inference\n",
    "def predict_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Extract features with DINOv2\n",
    "    with torch.no_grad():\n",
    "        features = dinov2(**inputs).last_hidden_state[:, 0].squeeze().cpu()\n",
    "    \n",
    "    # Get prediction from linear classifier\n",
    "    with torch.no_grad():\n",
    "        outputs = model(features)\n",
    "    \n",
    "    logits = outputs[\"logits\"]\n",
    "    predicted_class_id = logits.argmax(-1).item()\n",
    "    predicted_class = f\"stage_{predicted_class_id}\"\n",
    "    \n",
    "    return predicted_class, torch.softmax(logits, dim=-1)[predicted_class_id].item()\n",
    "\n",
    "# Test on a sample image\n",
    "sample_stage = \"stage_0\"\n",
    "sample_image = os.listdir(os.path.join(image_dir, sample_stage))[0]\n",
    "sample_path = os.path.join(image_dir, sample_stage, sample_image)\n",
    "\n",
    "predicted_class, confidence = predict_image(sample_path)\n",
    "print(f\"Sample image: {sample_path}\")\n",
    "print(f\"Predicted class: {predicted_class}, confidence: {confidence:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dslab_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
