{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from transformers import AutoImageProcessor\n",
    "from train import DINOv2Classifier\n",
    "from safetensors.torch import load_file as load_safetensors\n",
    "from IPython.display import display\n",
    "\n",
    "# CONFIG\n",
    "repo_dir = os.getcwd().split('dslab25')[0] + 'dslab25/'\n",
    "base_dir = os.path.join(repo_dir, \"training/vacuum_pump\")\n",
    "video_path = os.path.join(repo_dir, \"assets/vacuum_pump/videos/01_run1_cam_2_1024x1024_15fps_3mbps.mp4\")\n",
    "labels_path = os.path.join(repo_dir, \"assets/vacuum_pump/videos/output.txt\")\n",
    "model_dir = os.path.join(repo_dir, \"obj_detection/dino/dinov2_finetune/checkpoint-14760/\")\n",
    "coco_path = os.path.join(base_dir, \"coco_annotations.json\")\n",
    "pretrained_model = \"facebook/dinov2-with-registers-large\"\n",
    "def load_labels(labels_path):\n",
    "\t\"\"\"Load ground truth labels from file\"\"\"\n",
    "\tframe_to_class = {}\n",
    "\twith open(labels_path, 'r') as f:\n",
    "\t\tfor line in f:\n",
    "\t\t\tparts = line.strip().split()\n",
    "\t\t\tif len(parts) == 3:\n",
    "\t\t\t\tstate_class, start_frame, end_frame = int(parts[0]), int(parts[1]), int(parts[2])\n",
    "\t\t\t\tfor frame_idx in range(start_frame, end_frame + 1):  # +1 to include end_frame\n",
    "\t\t\t\t\tframe_to_class[frame_idx] = state_class\n",
    "\treturn frame_to_class\n",
    "\n",
    "# Load ground truth labels\n",
    "print(f\"Loading labels from: {labels_path}\")\n",
    "frame_to_class = load_labels(labels_path)\n",
    "\n",
    "# Load categories from COCO file for class names\n",
    "print(f\"Loading COCO annotations from: {coco_path}\")\n",
    "try:\n",
    "\twith open(coco_path, 'r') as f:\n",
    "\t\tcoco_data = json.load(f)\n",
    "\tcategory_id_to_name = {cat['id']: cat.get('name', f'category_{cat[\"id\"]}') \n",
    "\t\t\t\t\t\t\tfor cat in coco_data.get('categories', [])}\n",
    "except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "\tprint(f\"Error loading COCO file: {e}\")\n",
    "\t# Default to numeric IDs if COCO file not available\n",
    "\tcategory_id_to_name = {}\n",
    "\n",
    "# Load video\n",
    "print(f\"Loading video from: {video_path}\")\n",
    "video = cv2.VideoCapture(video_path)\n",
    "if not video.isOpened():\n",
    "\traise Exception(f\"Error: Could not open video at {video_path}\")\n",
    "\t\n",
    "total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "fps = video.get(cv2.CAP_PROP_FPS)\n",
    "print(f\"Video info: {total_frames} frames, {fps} fps\")\n",
    "\n",
    "# Load image processor and model\n",
    "print(\"Loading image processor...\")\n",
    "processor = AutoImageProcessor.from_pretrained(pretrained_model)\n",
    "\n",
    "# Determine device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Get number of classes from frame_to_class\n",
    "num_labels = max(frame_to_class.values()) + 1 if frame_to_class else 8\n",
    "print(f\"Number of classes: {num_labels}\")\n",
    "\n",
    "# Load model\n",
    "print(\"Loading model...\")\n",
    "model = DINOv2Classifier(num_labels=num_labels, pretrained_model=pretrained_model)\n",
    "\n",
    "# Load model weights\n",
    "safetensors_path = os.path.join(model_dir, \"model.safetensors\")\n",
    "bin_path = os.path.join(model_dir, \"pytorch_model.bin\")\n",
    "\n",
    "model_weights_path = None\n",
    "if os.path.exists(safetensors_path):\n",
    "\tmodel_weights_path = safetensors_path\n",
    "elif os.path.exists(bin_path):\n",
    "\tmodel_weights_path = bin_path\n",
    "\t\n",
    "if model_weights_path:\n",
    "\tprint(f\"Loading model weights from: {model_weights_path}\")\n",
    "\ttry:\n",
    "\t\tif model_weights_path.endswith(\".safetensors\"):\n",
    "\t\t\tstate_dict = load_safetensors(model_weights_path, device=str(device))\n",
    "\t\telse:\n",
    "\t\t\tstate_dict = torch.load(model_weights_path, map_location=str(device), weights_only=True)\n",
    "\t\t\t\n",
    "\t\t# Handle potential DDP prefix\n",
    "\t\tif next(iter(state_dict)).startswith('module.'):\n",
    "\t\t\tstate_dict = {k.partition('module.')[2]: v for k,v in state_dict.items()}\n",
    "\t\t\t\t\n",
    "\t\tmodel.load_state_dict(state_dict)\n",
    "\texcept Exception as e:\n",
    "\t\traise e\n",
    "else:\n",
    "\traise Exception(f\"Error: Model weights not found in {model_dir}\")\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Process frames\n",
    "print(\"\\n--- Starting Evaluation ---\")\n",
    "frame_idx = 0\n",
    "frames_to_process = []\n",
    "\n",
    "# Collect frames to process (every 5th frame)\n",
    "while True:\n",
    "\tret, frame = video.read()\n",
    "\tif not ret:\n",
    "\t\tbreak\n",
    "\t\n",
    "\tif frame_idx % 5 == 0:  # Process every 5th frame\n",
    "\t\tif frame_idx in frame_to_class:  # Only process frames with labels\n",
    "\t\t\tframes_to_process.append((frame_idx, frame))\n",
    "\t\n",
    "\tframe_idx += 1\n",
    "\n",
    "video.release()\n",
    "print(f\"Total frames to evaluate: {len(frames_to_process)}\")\n",
    "\n",
    "# Evaluate model on selected frames# Evaluate model on selected frames\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "for frame_idx, frame in frames_to_process[150:200]:\n",
    "\ttrue_label = frame_to_class[frame_idx]\n",
    "\t\n",
    "\t# Convert frame from BGR to RGB and then to PIL Image\n",
    "\t# Convert frame from BGR to RGB and then to PIL Image\n",
    "\tframe_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\t# Black out bottom 1/3\n",
    "\theight = frame_rgb.shape[0]\n",
    "\tblack_start_row = height * 2 // 3\n",
    "\tframe_rgb[black_start_row:, :] = 0\n",
    "\n",
    "\t# Convert to PIL Image\n",
    "\timage = Image.fromarray(frame_rgb)\n",
    "\tdraw = ImageDraw.Draw(image)\n",
    "\n",
    "\t# Convert back to BGR for YOLO (since YOLO model expects BGR OpenCV image)\n",
    "\tframe_rgb = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "\t\n",
    "\t\n",
    "\t# Show the image frame in notebook\n",
    "\tdisplay(image)\n",
    "\t\n",
    "\t# Process image\n",
    "\tinputs = processor(images=image, return_tensors=\"pt\")\n",
    "\tpixel_values = inputs['pixel_values'].to(device)\n",
    "\t\n",
    "\t# Make prediction\n",
    "\twith torch.no_grad():\n",
    "\t\toutputs = model(pixel_values=pixel_values)\n",
    "\t\n",
    "\tlogits = outputs['logits']\n",
    "\tprint(logits)\n",
    "\tpredicted_label = logits.argmax(-1).item()\n",
    "\t\n",
    "\t# Get class names\n",
    "\ttrue_label_name = category_id_to_name.get(true_label, f\"Class_{true_label}\")\n",
    "\tpredicted_label_name = category_id_to_name.get(predicted_label, f\"Class_{predicted_label}\")\n",
    "\t\n",
    "\t# Log result\n",
    "\tis_correct = predicted_label == true_label\n",
    "\tif is_correct:\n",
    "\t\tcorrect_predictions += 1\n",
    "\ttotal_predictions += 1\n",
    "\t\n",
    "\tprint(f\"Frame {frame_idx}:\")\n",
    "\tprint(f\"  True label: {true_label_name} (ID: {true_label})\")\n",
    "\tprint(f\"  Distribution: {logits}\")\n",
    "\tprint(f\"  Predicted: {predicted_label_name} (ID: {predicted_label})\")\n",
    "\tprint(f\"  Correct: {'✅ Yes' if is_correct else '❌ No'}\")\n",
    "\tprint(\"-\" * 40)\n",
    "print(\"correct predictions: \", correct_predictions)\n",
    "print(\"total predictions: \", total_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "Saved 357 labeled frames to:\n",
      "  Images: /workspace/dslab25/assets/vacuum_pump/eval/image\n",
      "  Labels: /workspace/dslab25/assets/vacuum_pump/class\n"
     ]
    }
   ],
   "source": [
    "def save_labeled_frames(video_path, labels_path, output_image_dir, output_label_dir):\n",
    "\tos.makedirs(output_image_dir, exist_ok=True)\n",
    "\tos.makedirs(output_label_dir, exist_ok=True)\n",
    "\n",
    "\t# Load label mapping\n",
    "\tframe_to_class = load_labels(labels_path)\n",
    "\n",
    "\t# Load video\n",
    "\tvideo = cv2.VideoCapture(video_path)\n",
    "\tif not video.isOpened():\n",
    "\t\traise Exception(f\"Error: Could not open video at {video_path}\")\n",
    "\n",
    "\tframe_idx = 0\n",
    "\tsaved_count = 0\n",
    "\n",
    "\twhile True:\n",
    "\t\tret, frame = video.read()\n",
    "\t\tif not ret:\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\tif frame_idx % 5 == 0 and frame_idx in frame_to_class:\n",
    "\t\t\tclass_id = frame_to_class[frame_idx]\n",
    "\t\t\t# Save image\n",
    "\t\t\timage_path = os.path.join(output_image_dir, f\"frame_{frame_idx}.jpg\")\n",
    "\t\t\tcv2.imwrite(image_path, frame)\n",
    "\n",
    "\t\t\t# Save label\n",
    "\t\t\tlabel_path = os.path.join(output_label_dir, f\"frame_{frame_idx}.txt\")\n",
    "\t\t\twith open(label_path, 'w') as label_file:\n",
    "\t\t\t\tlabel_file.write(str(class_id))\n",
    "\n",
    "\t\t\tsaved_count += 1\n",
    "\n",
    "\t\tframe_idx += 1\n",
    "\n",
    "\tvideo.release()\n",
    "\tprint(f\"Saved {saved_count} labeled frames to:\")\n",
    "\tprint(f\"  Images: {output_image_dir}\")\n",
    "\tprint(f\"  Labels: {output_label_dir}\")\n",
    "save_labeled_frames(\n",
    "\tvideo_path=video_path,\n",
    "\tlabels_path=labels_path,\n",
    "\toutput_image_dir=os.path.join(repo_dir, \"assets/vacuum_pump/eval/image\"),\n",
    "\toutput_label_dir=os.path.join(repo_dir, \"assets/vacuum_pump/eval/class\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
