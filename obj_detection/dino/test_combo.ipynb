{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "from transformers import AutoImageProcessor\n",
    "from train import DINOv2Classifier  # Your defined classifier\n",
    "from safetensors.torch import load_file as load_safetensors\n",
    "from IPython.display import display\n",
    "\n",
    "# CONFIGURATION\n",
    "repo_dir = os.getcwd().split('dslab25')[0] + 'dslab25/'\n",
    "base_dir = os.path.join(repo_dir, \"training/vacuum_pump\")\n",
    "video_path = os.path.join(repo_dir, \"assets/vacuum_pump/videos/01_run1_cam_2_1024x1024_15fps_3mbps.mp4\")\n",
    "labels_path = os.path.join(repo_dir, \"assets/vacuum_pump/videos/output.txt\")\n",
    "model_dir = os.path.join(repo_dir, \"obj_detection/dino/dinov2_finetune/final_model/\")\n",
    "coco_path = os.path.join(base_dir, \"coco_annotations.json\")\n",
    "\n",
    "# YOLO model path â€“ adjust according to where your trained YOLO weights are saved.\n",
    "yolo_model_path = os.path.join(repo_dir, \"object_detection/dino/yolo_runs/yolov12_boundingbox\", \"weights\", \"best.pt\")\n",
    "\n",
    "def load_labels(labels_path):\n",
    "\t\"\"\"Load ground truth labels from file.\"\"\"\n",
    "\tframe_to_class = {}\n",
    "\twith open(labels_path, 'r') as f:\n",
    "\t\tfor line in f:\n",
    "\t\t\tparts = line.strip().split()\n",
    "\t\t\tif len(parts) == 3:\n",
    "\t\t\t\tstate_class, start_frame, end_frame = int(parts[0]), int(parts[1]), int(parts[2])\n",
    "\t\t\t\tfor frame_idx in range(start_frame, end_frame + 1):\n",
    "\t\t\t\t\tframe_to_class[frame_idx] = state_class\n",
    "\treturn frame_to_class\n",
    "\n",
    "def main():\n",
    "\t# Load ground truth labels.\n",
    "\tprint(f\"Loading labels from: {labels_path}\")\n",
    "\tframe_to_class = load_labels(labels_path)\n",
    "\t\n",
    "\t# Load COCO annotations to get class names.\n",
    "\tprint(f\"Loading COCO annotations from: {coco_path}\")\n",
    "\ttry:\n",
    "\t\twith open(coco_path, 'r') as f:\n",
    "\t\t\tcoco_data = json.load(f)\n",
    "\t\tcategory_id_to_name = {cat['id']: cat.get('name', f'category_{cat[\"id\"]}')\n",
    "\t\t\t\t\t\t\t\t for cat in coco_data.get('categories', [])}\n",
    "\texcept (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "\t\tprint(f\"Error loading COCO file: {e}\")\n",
    "\t\tcategory_id_to_name = {}\n",
    "\t\n",
    "\t# Open video file.\n",
    "\tprint(f\"Loading video from: {video_path}\")\n",
    "\tvideo = cv2.VideoCapture(video_path)\n",
    "\tif not video.isOpened():\n",
    "\t\tprint(f\"Error: Could not open video at {video_path}\")\n",
    "\t\treturn\n",
    "\ttotal_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\tfps = video.get(cv2.CAP_PROP_FPS)\n",
    "\tprint(f\"Video info: {total_frames} frames, {fps} fps\")\n",
    "\t\n",
    "\t# Load image processor for DinoV2.\n",
    "\tprint(\"Loading image processor for DinoV2...\")\n",
    "\tprocessor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-with-registers-base\")\n",
    "\t\n",
    "\t# Set device.\n",
    "\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\tprint(f\"Using device: {device}\")\n",
    "\t\n",
    "\t# Determine number of classes.\n",
    "\tnum_labels = max(frame_to_class.values()) + 1 if frame_to_class else 8\n",
    "\tprint(f\"Number of classes: {num_labels}\")\n",
    "\t\n",
    "\t# Load the DinoV2 classifier.\n",
    "\tprint(\"Loading DinoV2 classifier model...\")\n",
    "\tdino_model = DINOv2Classifier(num_labels=num_labels)\n",
    "\t\n",
    "\t# Load DinoV2 model weights.\n",
    "\tsafetensors_path = os.path.join(model_dir, \"model.safetensors\")\n",
    "\tbin_path = os.path.join(model_dir, \"pytorch_model.bin\")\n",
    "\tmodel_weights_path = None\n",
    "\tif os.path.exists(safetensors_path):\n",
    "\t\tmodel_weights_path = safetensors_path\n",
    "\telif os.path.exists(bin_path):\n",
    "\t\tmodel_weights_path = bin_path\n",
    "\t\t\n",
    "\tif model_weights_path:\n",
    "\t\tprint(f\"Loading DinoV2 model weights from: {model_weights_path}\")\n",
    "\t\ttry:\n",
    "\t\t\tif model_weights_path.endswith(\".safetensors\"):\n",
    "\t\t\t\tstate_dict = load_safetensors(model_weights_path, device=str(device))\n",
    "\t\t\telse:\n",
    "\t\t\t\tstate_dict = torch.load(model_weights_path, map_location=str(device), weights_only=True)\n",
    "\t\t\t# Remove potential DDP prefix\n",
    "\t\t\tif next(iter(state_dict)).startswith('module.'):\n",
    "\t\t\t\tstate_dict = {k.partition('module.')[2]: v for k, v in state_dict.items()}\n",
    "\t\t\tdino_model.load_state_dict(state_dict)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f\"Error loading DinoV2 model weights: {e}\")\n",
    "\t\t\treturn\n",
    "\telse:\n",
    "\t\tprint(f\"Error: DinoV2 model weights not found in {model_dir}\")\n",
    "\t\treturn\n",
    "\t\n",
    "\tdino_model.to(device)\n",
    "\tdino_model.eval()\n",
    "\t\n",
    "\t# Load the YOLOv12 model.\n",
    "\tprint(\"Loading YOLOv12 model for bounding box extraction...\")\n",
    "\tyolo_model = YOLO(yolo_model_path)\n",
    "\t# If supported, you can move the YOLO model to the same device:\n",
    "\t# yolo_model.to(device)\n",
    "\t\n",
    "\tprint(\"\\n--- Starting Evaluation ---\")\n",
    "\tframe_idx = 0\n",
    "\tframes_to_process = []\n",
    "\twhile True:\n",
    "\t\tret, frame = video.read()\n",
    "\t\tif not ret:\n",
    "\t\t\tbreak\n",
    "\t\t# Process every 5th frame that has a label.\n",
    "\t\tif frame_idx % 5 == 0 and frame_idx in frame_to_class:\n",
    "\t\t\tframes_to_process.append((frame_idx, frame))\n",
    "\t\tframe_idx += 1\n",
    "\tvideo.release()\n",
    "\tprint(f\"Total frames to evaluate: {len(frames_to_process)}\")\n",
    "\t\n",
    "\tcorrect_predictions = 0\n",
    "\ttotal_predictions = 0\n",
    "\t\n",
    "\t# For demonstration, we process a subset (frames 20 to 40).\n",
    "\tfor frame_idx, frame in frames_to_process[20:40]:\n",
    "\t\ttrue_label = frame_to_class[frame_idx]\n",
    "\t\t\n",
    "\t\t# Run YOLO on the frame to extract bounding boxes.\n",
    "\t\tyolo_results = yolo_model(frame)\n",
    "\t\tif len(yolo_results) == 0 or len(yolo_results[0].boxes) == 0:\n",
    "\t\t\tprint(f\"Frame {frame_idx}: No bounding box detected. Skipping frame.\")\n",
    "\t\t\tcontinue\n",
    "\t\t\n",
    "\t\t# Choose the bounding box with the highest confidence.\n",
    "\t\tboxes = yolo_results[0].boxes\n",
    "\t\tbox_data = boxes.data  # Assumes tensor with columns [x1, y1, x2, y2, conf, cls]\n",
    "\t\tidx = torch.argmax(box_data[:, 4])\n",
    "\t\tbox = box_data[idx]\n",
    "\t\tx1, y1, x2, y2 = map(int, box[:4].tolist())\n",
    "\t\t# Clamp coordinates to frame dimensions.\n",
    "\t\th, w, _ = frame.shape\n",
    "\t\tx1, y1 = max(0, x1), max(0, y1)\n",
    "\t\tx2, y2 = min(w, x2), min(h, y2)\n",
    "\t\t\n",
    "\t\t# Crop the detected region.\n",
    "\t\tcropped = frame[y1:y2, x1:x2]\n",
    "\t\tif cropped.size == 0:\n",
    "\t\t\tprint(f\"Frame {frame_idx}: Cropped region is empty. Skipping frame.\")\n",
    "\t\t\tcontinue\n",
    "\t\tcropped_rgb = cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB)\n",
    "\t\tcropped_image = Image.fromarray(cropped_rgb)\n",
    "\t\t\n",
    "\t\t# Preprocess the cropped image and classify using DinoV2.\n",
    "\t\tinputs = processor(images=cropped_image, return_tensors=\"pt\")\n",
    "\t\tpixel_values = inputs['pixel_values'].to(device)\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\toutputs = dino_model(pixel_values=pixel_values)\n",
    "\t\tlogits = outputs['logits']\n",
    "\t\tpredicted_label = logits.argmax(-1).item()\n",
    "\t\t\n",
    "\t\t# Map numerical labels to names.\n",
    "\t\ttrue_label_name = category_id_to_name.get(true_label, f\"Class_{true_label}\")\n",
    "\t\tpredicted_label_name = category_id_to_name.get(predicted_label, f\"Class_{predicted_label}\")\n",
    "\t\t\n",
    "\t\tis_correct = predicted_label == true_label\n",
    "\t\tif is_correct:\n",
    "\t\t\tcorrect_predictions += 1\n",
    "\t\ttotal_predictions += 1\n",
    "\n",
    "\t\t# Convert frame from BGR to RGB and then to PIL Image\n",
    "\t\tframe_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\t\timage = Image.fromarray(frame_rgb)\n",
    "\n",
    "\t\t# Show the image frame in notebook\n",
    "\t\tdisplay(image)\n",
    "\t\tdisplay(cropped_image)\n",
    "\t\t\n",
    "\t\tprint(f\"Frame {frame_idx}:\")\n",
    "\t\tprint(f\"  True label: {true_label_name} (ID: {true_label})\")\n",
    "\t\tprint(f\"  Predicted: {predicted_label_name} (ID: {predicted_label})\")\n",
    "\t\tprint(f\"  Correct: {'Yes' if is_correct else 'No'}\")\n",
    "\t\tprint(\"-\" * 20)\n",
    "\t\n",
    "\taccuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "\tprint(f\"\\nEvaluation Summary:\")\n",
    "\tprint(f\"  Total frames evaluated: {total_predictions}\")\n",
    "\tprint(f\"  Correct predictions: {correct_predictions}\")\n",
    "\tprint(f\"  Accuracy: {accuracy:.2f} ({correct_predictions}/{total_predictions})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tmain()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
