{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"HF_HOME\"] = \"/workspace/huggingface\"\n",
    "# or\n",
    "# os.environ[\"TRANSFORMERS_CACHE\"] = \"/workspace/hf_models\"\n",
    "\n",
    "import base64\n",
    "import io\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from transformers import AutoImageProcessor\n",
    "from utils import DINOv2Classifier\n",
    "from safetensors.torch import load_file as load_safetensors\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# CONFIGURATION\n",
    "repo_dir = os.getcwd().split('dslab25')[0] + 'dslab25/'\n",
    "video_path = os.path.join(repo_dir, \"assets/vacuum_pump/videos/01_run1_cam_2_1024x1024_15fps_3mbps.mp4\")\n",
    "labels_path = os.path.join(repo_dir, \"assets/vacuum_pump/videos/output.txt\")\n",
    "coco_path = os.path.join(repo_dir, \"training/vacuum_pump/coco_annotations.json\")\n",
    "\n",
    "# Path to your trained YOLOv12 weights (adjust as needed)\n",
    "temp_images_dir = os.path.join(repo_dir, \"temp_images\")\n",
    "anno_dir = os.path.join(repo_dir, \"assets/vacuum_pump/eval/anno\")\n",
    "base_dir = os.path.join(repo_dir, \"training/vacuum_pump\")\n",
    "\n",
    "coco_path = os.path.join(base_dir, \"coco_annotations.json\")\n",
    "YOL_THRESHOLD = 0.38\n",
    "\n",
    "os.makedirs(temp_images_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = \"facebook/dinov2-with-registers-base\"\n",
    "yolo_model_path = os.path.join(repo_dir, \"obj_detection/dino/yolo_runs/yolov12_boundingbox2\", \"weights\", \"best.pt\")\n",
    "model_dir = os.path.join(repo_dir, \"obj_detection/dino/dinov2_finetune/base/final_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_labels(labels_path):\n",
    "\t\"\"\"Load ground truth labels from file.\"\"\"\n",
    "\tframe_to_class = {}\n",
    "\twith open(labels_path, 'r') as f:\n",
    "\t\tfor line in f:\n",
    "\t\t\tparts = line.strip().split()\n",
    "\t\t\tif len(parts) == 3:\n",
    "\t\t\t\tstate_class, start_frame, end_frame = int(parts[0]), int(parts[1]), int(parts[2])\n",
    "\t\t\t\tfor frame_idx in range(start_frame, end_frame + 1):\n",
    "\t\t\t\t\tframe_to_class[frame_idx] = state_class\n",
    "\treturn frame_to_class\n",
    "\n",
    "# Load ground truth labels.\n",
    "print(f\"Loading labels from: {labels_path}\")\n",
    "frame_to_class = load_labels(labels_path)\n",
    "\n",
    "# Load COCO annotations to map category IDs to names.\n",
    "print(f\"Loading COCO annotations from: {coco_path}\")\n",
    "try:\n",
    "\twith open(coco_path, 'r') as f:\n",
    "\t\tcoco_data = json.load(f)\n",
    "\tcategory_id_to_name = {cat['id']: cat.get('name', f'category_{cat[\"id\"]}')\n",
    "\t\t\t\t\t\t\t\tfor cat in coco_data.get('categories', [])}\n",
    "except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "\tprint(f\"Error loading COCO annotations: {e}\")\n",
    "\tcategory_id_to_name = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label grouping logic\n",
    "def labels_match(pred, true):\n",
    "\tgroup_1 = {4, 5}\n",
    "\tgroup_2 = {6, 7}\n",
    "\tif pred == true:\n",
    "\t\treturn True\n",
    "\t# return False\n",
    "\tif pred in group_1 and true in group_1:\n",
    "\t\treturn True\n",
    "\tif pred in group_2 and true in group_2:\n",
    "\t\treturn True\n",
    "\treturn False\n",
    "\n",
    "try:\n",
    "\tfont = ImageFont.truetype(\"arial.ttf\", 16)\n",
    "except IOError:\n",
    "\tfont = ImageFont.load_default()\n",
    "\n",
    "print(f\"Loading video from: {video_path}\")\n",
    "video = cv2.VideoCapture(video_path)\n",
    "if not video.isOpened():\n",
    "\traise Exception(\"Error: Could not open video file.\")\n",
    "\t\n",
    "total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "fps = video.get(cv2.CAP_PROP_FPS)\n",
    "print(f\"Video info: {total_frames} frames, {fps} fps\")\n",
    "\n",
    "print(\"\\n--- Starting YOLO Evaluation ---\")\n",
    "frame_idx = 0\n",
    "frames_to_process = []\n",
    "while True:\n",
    "\tret, frame = video.read()\n",
    "\tif not ret:\n",
    "\t\tbreak\n",
    "\tif frame_idx % 5 == 0 and frame_idx in frame_to_class:\n",
    "\t\tframes_to_process.append((frame_idx, frame))\n",
    "\tframe_idx += 1\n",
    "video.release()\n",
    "print(f\"Total frames to evaluate: {len(frames_to_process)}\")\n",
    "\n",
    "os.makedirs(temp_images_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading YOLO model...\")\n",
    "yolo_model = YOLO(yolo_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "for frame_idx, frame in frames_to_process:\n",
    "\tframe_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\timage = Image.fromarray(frame_rgb)\n",
    "\n",
    "\tyolo_results = yolo_model(frame)\n",
    "\tif len(yolo_results) == 0 or len(yolo_results[0].boxes) == 0:\n",
    "\t\tprint(f\"Frame {frame_idx}: No detection found. Skipping frame.\")\n",
    "\t\tcontinue\n",
    "\n",
    "\tboxes = yolo_results[0].boxes.data\t# Each row: [x1, y1, x2, y2, conf, cls]\n",
    "\ttrue_label = frame_to_class[frame_idx]\n",
    "\ttrue_label_name = category_id_to_name.get(true_label, f\"Class_{true_label}\")\n",
    "\t# Filter boxes by threshold\n",
    "\tfiltered_boxes = [box for box in boxes if box[4].item() >= YOL_THRESHOLD]\n",
    "\n",
    "\t# Pick the box with the highest confidence\n",
    "\tif filtered_boxes:\n",
    "\t\tbest_box = max(filtered_boxes, key=lambda b: b[4].item())\n",
    "\t\tconfidence = best_box[4].item()\n",
    "\n",
    "\t\tpredicted_label = int(best_box[5].item())\n",
    "\t\tpredicted_label_name = category_id_to_name.get(predicted_label, f\"Class_{predicted_label}\")\n",
    "\n",
    "\t\tx1, y1, x2, y2 = map(int, best_box[:4].tolist())\n",
    "\t\tx1 = max(0, x1)\n",
    "\t\ty1 = max(0, y1)\n",
    "\t\tx2 = min(image.width, x2)\n",
    "\t\ty2 = min(image.height, y2)\n",
    "\n",
    "\t\tcropped_image = image.crop((x1, y1, x2, y2))\n",
    "\t\tcropped_path = os.path.join(temp_images_dir, f\"frame_{str(frame_idx).zfill(4)}.jpg\")\n",
    "\t\tcropped_image.save(cropped_path)\n",
    "\n",
    "\t\tprint(f\"Saved cropped image: {cropped_path}\")\n",
    "\n",
    "\t\tis_correct = labels_match(predicted_label, true_label)\n",
    "\t\tif is_correct:\n",
    "\t\t\t\tcorrect_predictions += 1\n",
    "\t\ttotal_predictions += 1\n",
    "\n",
    "\t\tprint(f\"Frame {frame_idx}\")\n",
    "\t\tprint(f\"\tTrue:\t  {true_label_name} (ID: {true_label})\")\n",
    "\t\tprint(f\"\tPredicted: {predicted_label_name} (ID: {predicted_label}) | Conf: {confidence:.2f}\")\n",
    "\t\tprint(f\"\tCorrect:   {'✅ Yes' if is_correct else '❌ No'}\")\n",
    "\t\tprint(\"-\" * 30)\n",
    "\n",
    "accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "print(\"\\nEvaluation Summary:\")\n",
    "print(f\"\tTotal predictions: {total_predictions}\")\n",
    "print(f\"\tCorrect predictions: {correct_predictions}\")\n",
    "print(f\"\tAccuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, imageio, numpy as np, torch, torch.nn.functional as F\n",
    "from IPython.display import Video\n",
    "from sam2.sam2_image_predictor  import SAM2ImagePredictor\n",
    "from sam2.sam2_video_predictor  import SAM2VideoPredictor\n",
    "from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n",
    "from transformers import AutoModel, AutoImageProcessor\n",
    "\n",
    "# 2.  PATHS & OPTIONS  ---------------------------------------------------------refs        = [\n",
    "refs = [\n",
    "    os.path.join(base_dir, \"images/original/stage_0/stage_0_var_0_case_render_1.jpg\"),\n",
    "    os.path.join(base_dir, \"images/original/stage_0/stage_0_var_0_case_render_2.jpg\"),\n",
    "    os.path.join(base_dir, \"images/original/stage_0/stage_0_var_0_case_render_3.jpg\"),\n",
    "    os.path.join(base_dir, \"images/original/stage_0/stage_0_var_0_case_render_4.jpg\"),\n",
    "    os.path.join(base_dir, \"images/original/stage_0/stage_0_var_0_case_render_5.jpg\"),\n",
    "    os.path.join(base_dir, \"images/original/stage_0/stage_0_var_0_case_render_6.jpg\"),\n",
    "    os.path.join(base_dir, \"images/original/stage_0/stage_0_var_0_case_render_7.jpg\"),\n",
    "    os.path.join(base_dir, \"images/original/stage_0/stage_0_var_0_case_render_8.jpg\"),\n",
    "    os.path.join(base_dir, \"images/original/stage_0/stage_0_var_0_case_render_9.jpg\")\n",
    "]\n",
    "boxed_out   = \"sam2_boxed.mp4\"\n",
    "device      = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "skip_frames = 70                             # ← skip these many frames\n",
    "\n",
    "# ── 2. UTILS ─────────────────────────────────────────────────────────────────\n",
    "def read_video_rgb(path):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(f\"cannot open {path}\")\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frm = cap.read()\n",
    "        if not ret: break\n",
    "        frames.append(cv2.cvtColor(frm, cv2.COLOR_BGR2RGB))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or 30\n",
    "    cap.release()\n",
    "    return frames, int(fps)\n",
    "\n",
    "def get_feat(img_rgb):\n",
    "    ipt = dinov2_proc(images=img_rgb, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = dinov2_backbone(**ipt).last_hidden_state[:,0]\n",
    "    return F.normalize(out.squeeze(0), dim=-1).cpu()\n",
    "\n",
    "# ── 3. LOAD VIDEO ────────────────────────────────────────────────────────────\n",
    "frames, FPS = read_video_rgb(video_path)\n",
    "assert len(frames) > skip_frames, f\"Video must have more than {skip_frames} frames!\"\n",
    "H, W = frames[0].shape[:2]\n",
    "\n",
    "# ── 4. INITIALISE MODELS ────────────────────────────────────────────────────\n",
    "backbone_name   = \"facebook/dinov2-with-registers-small\"\n",
    "dinov2_backbone = AutoModel.from_pretrained(backbone_name).to(device).eval()\n",
    "dinov2_proc     = AutoImageProcessor.from_pretrained(backbone_name)\n",
    "\n",
    "img_pred = SAM2ImagePredictor.from_pretrained(\"facebook/sam2.1-hiera-tiny\")\n",
    "vid_pred = SAM2VideoPredictor.from_pretrained(\"facebook/sam2.1-hiera-tiny\")\n",
    "\n",
    "# ── 5. REFERENCE EMBEDDINGS ─────────────────────────────────────────────────\n",
    "ref_feats = []\n",
    "for p in refs:\n",
    "    bgr = cv2.imread(p)\n",
    "    if bgr is None:\n",
    "        raise IOError(f\"cannot open reference image {p}\")\n",
    "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "    ref_feats.append(get_feat(rgb))\n",
    "\n",
    "# ── 6. AUTO MASKS ON SEED FRAME ─────────────────────────────────────────────\n",
    "seed_frame = frames[skip_frames]\n",
    "mask_gen = SAM2AutomaticMaskGenerator(\n",
    "    img_pred.model,\n",
    "    points_per_side=32,\n",
    "    pred_iou_thresh=0.7,          # only keep masks with IoU-pred confidence ≥ 0.7\n",
    "    stability_score_thresh=0.9,   # only keep very stable masks\n",
    "    box_nms_thresh=0.3,           # merge overlapping boxes more aggressively\n",
    "    min_mask_region_area=1000     # drop very small regions\n",
    ")\n",
    "masks = mask_gen.generate(seed_frame)\n",
    "\n",
    "# ── 7. PICK BEST MASK BY COS-SIM ────────────────────────────────────────────\n",
    "best_m, best_sim = None, -1.0\n",
    "for m in masks:\n",
    "    x_f, y_f, w_f, h_f = m[\"bbox\"]\n",
    "    x0 = max(0, int(round(x_f)));     y0 = max(0, int(round(y_f)))\n",
    "    x1 = min(W, int(round(x_f + w_f))); y1 = min(H, int(round(y_f + h_f)))\n",
    "    if x1 <= x0 or y1 <= y0:\n",
    "        continue\n",
    "\n",
    "    crop = seed_frame[y0:y1, x0:x1]\n",
    "    if crop.size == 0:\n",
    "        continue\n",
    "\n",
    "    feat = get_feat(crop)\n",
    "    sims = torch.stack(ref_feats) @ feat     # [n_refs]\n",
    "    sim  = sims.max().item()\n",
    "    if sim > best_sim:\n",
    "        best_m, best_sim = m, sim\n",
    "\n",
    "if best_m is None:\n",
    "    raise RuntimeError(\"No mask matched the reference images!\")\n",
    "\n",
    "mask0 = torch.from_numpy(best_m[\"segmentation\"]).to(device).bool()\n",
    "\n",
    "# ── 8. TRACK & DRAW BOX (SKIPPING FIRST 70 FRAMES) ──────────────────────────\n",
    "# --- use the MP4 path rather than a tensor ---\n",
    "state = vid_pred.init_state(video_path=video_path)\n",
    "\n",
    "vid_pred.add_new_mask(state, frame_idx=skip_frames, mask=mask0, obj_id=0)\n",
    "writer = imageio.get_writer(\n",
    "    boxed_out,\n",
    "    format=\"FFMPEG\",    # force the FFmpeg plugin\n",
    "    codec=\"libx264\",    # MP4/H.264 codec\n",
    "    fps=FPS,\n",
    "    ffmpeg_params=[\"-pix_fmt\", \"yuv420p\"]  # ensures broad compatibility\n",
    ")\n",
    "with torch.inference_mode(), torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "    for f_idx, _, logits in vid_pred.propagate_in_video(state):\n",
    "        if f_idx < skip_frames:\n",
    "            continue\n",
    "\n",
    "        # get the first object’s mask; this will be shape (1, H, W)\n",
    "        mask_prob = logits.sigmoid()[0]      \n",
    "\n",
    "        # squeeze to (H, W)\n",
    "        mask2d = mask_prob.squeeze(0)        \n",
    "\n",
    "        # threshold\n",
    "        bin_m = (mask2d > 0.5).cpu().numpy().astype(np.uint8)\n",
    "\n",
    "        frame = frames[f_idx].copy()\n",
    "        if bin_m.any():\n",
    "            ys, xs = np.where(bin_m)           # now this is 2-D\n",
    "            x0b, y0b, x1b, y1b = xs.min(), ys.min(), xs.max(), ys.max()\n",
    "            cv2.rectangle(frame, (x0b, y0b), (x1b, y1b), (0,255,0), 2)\n",
    "        writer.append_data(frame)\n",
    "writer.close()\n",
    "\n",
    "# ── 9. DISPLAY RESULT ───────────────────────────────────────────────────────\n",
    "Video(boxed_out, embed=True, width=min(W, 640))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DinoV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "image_files = sorted(glob(os.path.join(temp_images_dir, \"*.jpg\")))\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Load image processor and model\n",
    "print(\"Loading image processor...\")\n",
    "processor = AutoImageProcessor.from_pretrained(pretrained_model)\n",
    "\n",
    "# Determine device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Get number of classes from frame_to_class\n",
    "num_labels = max(frame_to_class.values()) + 1 if frame_to_class else 8\n",
    "print(f\"Number of classes: {num_labels}\")\n",
    "\n",
    "# Load model\n",
    "print(\"Loading model...\")\n",
    "model = DINOv2Classifier(num_labels=num_labels, pretrained_model=pretrained_model)\n",
    "\n",
    "# Load model weights\n",
    "safetensors_path = os.path.join(model_dir, \"model.safetensors\")\n",
    "bin_path = os.path.join(model_dir, \"pytorch_model.bin\")\n",
    "\n",
    "model_weights_path = None\n",
    "if os.path.exists(safetensors_path):\n",
    "\tmodel_weights_path = safetensors_path\n",
    "elif os.path.exists(bin_path):\n",
    "\tmodel_weights_path = bin_path\n",
    "\t\n",
    "if model_weights_path:\n",
    "\tprint(f\"Loading model weights from: {model_weights_path}\")\n",
    "\ttry:\n",
    "\t\tif model_weights_path.endswith(\".safetensors\"):\n",
    "\t\t\tstate_dict = load_safetensors(model_weights_path, device=str(device))\n",
    "\t\telse:\n",
    "\t\t\tstate_dict = torch.load(model_weights_path, map_location=str(device), weights_only=True)\n",
    "\t\t\t\n",
    "\t\t# Handle potential DDP prefix\n",
    "\t\tif next(iter(state_dict)).startswith('module.'):\n",
    "\t\t\tstate_dict = {k.partition('module.')[2]: v for k,v in state_dict.items()}\n",
    "\t\t\t\t\n",
    "\t\tmodel.load_state_dict(state_dict)\n",
    "\texcept Exception as e:\n",
    "\t\traise e\n",
    "else:\n",
    "\traise Exception(f\"Error: Model weights not found in {model_dir}\")\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "for image_path in image_files:\n",
    "\t# Infer annotation file path\n",
    "\tfilename = os.path.splitext(os.path.basename(image_path))[0]\n",
    "\tanno_path = os.path.join(anno_dir, f\"{filename}.txt\")\n",
    "\tprint(anno_path)\n",
    "\tif not os.path.exists(anno_path):\n",
    "\t\tprint(f\"Annotation not found for {filename}, skipping.\")\n",
    "\t\tcontinue\n",
    "\n",
    "\t# Read true class label\n",
    "\twith open(anno_path, \"r\") as f:\n",
    "\t\ttry:\n",
    "\t\t\ttrue_label = int(f.readline().strip())\n",
    "\t\texcept ValueError:\n",
    "\t\t\tprint(f\"Invalid label in {anno_path}, skipping.\")\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t# Load image\n",
    "\timage = Image.open(image_path).convert(\"RGB\")\n",
    "\tdisplay(image)\n",
    "\n",
    "\t# Process image\n",
    "\tinputs = processor(images=image, return_tensors=\"pt\")\n",
    "\tpixel_values = inputs[\"pixel_values\"].to(device)\n",
    "\twith torch.no_grad():\n",
    "\t\toutputs = model(pixel_values=pixel_values)\n",
    "\t\n",
    "\tlogits = outputs[\"logits\"]\n",
    "\tprobs = torch.softmax(logits, dim=-1)\n",
    "\tprint(\"Probabilities:\", probs)\n",
    "\tpredicted_label = logits.argmax(-1).item()\n",
    "\n",
    "\t# Inside your loop:\n",
    "\tis_correct = labels_match(predicted_label, true_label)\n",
    "\tif is_correct:\n",
    "\t\tcorrect_predictions += 1\n",
    "\ttotal_predictions += 1\n",
    "\n",
    "\tprint(f\"{filename}:\")\n",
    "\tprint(f\"\tTrue:\t{true_label}\")\n",
    "\tprint(f\"\tPredicted: {predicted_label}\")\n",
    "\tprint(f\"\tCorrect:\t {'✅ Yes' if is_correct else '❌ No'}\")\n",
    "\tprint(\"-\" * 30)\n",
    "\n",
    "accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "print(\"\\nEvaluation Summary:\")\n",
    "print(f\"\tTotal evaluated: {total_predictions}\")\n",
    "print(f\"\tCorrect:\t\t {correct_predictions}\")\n",
    "print(f\"\tAccuracy:\t\t{accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen VL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "\n",
    "states = {\n",
    "\t'state_0': 'Base block metal piece',\n",
    "\t'state_1': 'Cylinder metal piece which gets stick on the base block stage_0',\n",
    "\t'state_2': 'A Big metal piece which gets stick on the cylinder piece of stage_1',\n",
    "\t'state_3': 'A smaller thin metal piece which gets put onto the center of the big metal piece of stage_2',\n",
    "\t'state_4': 'A tiny metal ring which gets placed onto the center of the thing metal piece of stage_3',\n",
    "\t'state_5': '3 screws now get screwed onto the piece',\n",
    "\t'state_6': 'A darker metal plate now gets placed on top of the piece',\n",
    "\t'state_7': '5 screws now get screwed onto the piece',\n",
    "}\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "qwen_vl_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "\t\"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "\ttorch_dtype=torch.bfloat16,\n",
    "\t# attn_implementation=\"flash_attention_2\",\n",
    "\tdevice_map=\"auto\",\n",
    ")\n",
    "\n",
    "# default processer\n",
    "qwen_vl_processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "\n",
    "# Messages containing a local video path and a text query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Messages containing a local video path and a text query\n",
    "\n",
    "def display_images_horizontal(image_paths, size=(100, 100)):\n",
    "\thtml = '<div style=\"display: flex; flex-direction: row;\">'\n",
    "\tfor path in image_paths:\n",
    "\t\timg = Image.open(path).resize(size)\n",
    "\t\tbuffer = io.BytesIO()\n",
    "\t\timg.save(buffer, format='PNG')\n",
    "\t\timg_str = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "\t\thtml += f'<img src=\"data:image/png;base64,{img_str}\" style=\"margin:2px;\" />'\n",
    "\thtml += '</div>'\n",
    "\tdisplay(HTML(html))\n",
    "\t\n",
    "def qwen_vl(image_paths: list[str], current_dino_state: str, previous_qwen_state: str, previous_information: str):\n",
    "\t\"\"\"\n",
    "\t[\n",
    "\t\t\"file:///path/to/frame1.jpg\",\n",
    "\t\t\"file:///path/to/frame2.jpg\",\n",
    "\t\t\"file:///path/to/frame3.jpg\",\n",
    "\t\t\"file:///path/to/frame4.jpg\",\n",
    "\t],\n",
    "\t\"\"\"\n",
    "\tmessages = [\n",
    "\t\t{\n",
    "\t\t\t\"role\": \"user\",\n",
    "\t\t\t\"content\": [\n",
    "\t\t\t{\n",
    "\t\t\t\t\"type\": \"video\",\n",
    "\t\t\t\t\"video\": image_paths\n",
    "\t\t\t},\n",
    "\t\t\t{\n",
    "\t\t\t\t\"type\": \"text\",\n",
    "\t\t\t\t\"text\": f'''\n",
    "\t\t\t\t\tYou are given a short video clip of a person assembling an object. The clip is part of a longer clip of the person assembling the object.\n",
    "\t\t\t\t\tYour task is it to tell me what you see in the video and based on that and the previous action, some more information below make a prediction on which state the piece is at the end of the clip.\n",
    "\t\t\t\t\tNote that the clip may end while the person is still in the middle of an action.\n",
    "\t\t\t\t\tIf the state is in between two states, return the state it was in before, so if the person is moving the cylinder piece to the base block, return state_0, only when the cylinder piece is on the base block, return state_1.\n",
    "\t\t\t\t\tHere are the possible states:\n",
    "\t\t\t\t\t\t'state_0': 'First part of the object: Base block metal piece',\n",
    "\t\t\t\t\t\t'state_1': 'Second part of the object: Cylinder metal piece which gets stick on the base block stage_0',\n",
    "\t\t\t\t\t\t'state_2': 'Third part of the object: A Big metal piece which gets stick on the cylinder piece of stage_1',\n",
    "\t\t\t\t\t\t'state_3': 'Fourth part of the object: A smaller thin metal piece which gets put onto the center of the big metal piece of stage_2',\n",
    "\t\t\t\t\t\t'state_4': 'Fifth part of the object: A tiny metal ring which gets placed onto the center of the thing metal piece of stage_3',\n",
    "\t\t\t\t\t\t'state_5': 'Sixth part of the object: 3 screws now get screwed onto the piece',\n",
    "\t\t\t\t\t\t'state_6': 'Seventh part of the object: A darker metal plate now gets placed on top of the piece',\n",
    "\t\t\t\t\t\t'state_7': 'Eighth part of the object: 5 screws now get screwed onto the piece'\n",
    "\t\t\t\t\tAdditionally I will give you the state in which the classifier thinks the piece is at.\n",
    "\t\t\t\t\tHere is the information (it might be none id it didnt predict anything):\n",
    "\t\t\t\t\t\t{current_dino_state}\n",
    "\t\t\t\t\tAdditionally I will give you the state in which the classifier thinks the piece was at at the beginning of the clip.\n",
    "\t\t\t\t\tHere is the information ((it might be none id it didnt predict anything)):\n",
    "\t\t\t\t\t\t{previous_qwen_state}\n",
    "\t\t\t\t\tAdditionally, I will give you some information about what the person did 5 seconds before the clip starts.\n",
    "\t\t\t\t\tHere is the information:\n",
    "\t\t\t\t\t\t{previous_information}\n",
    "\n",
    "\t\t\t\t\tPlease return the output in the following format:\n",
    "\t\t\t\t\t{{\n",
    "\t\t\t\t\t\t\"state\": \"state_name\",\n",
    "\t\t\t\t\t\t\"action_description\": \"action_description\"\n",
    "\t\t\t\t\t}}\n",
    "\t\t\t\t'''},\n",
    "\t\t\t\t],\n",
    "\t\t\t}\n",
    "\t\t]\n",
    "\n",
    "\ttext = qwen_vl_processor.apply_chat_template(\n",
    "\t\tmessages, tokenize=False, add_generation_prompt=True\n",
    "\t)\n",
    "\timage_inputs, video_inputs = process_vision_info(messages)\n",
    "\tinputs = qwen_vl_processor(\n",
    "\t\ttext=[text],\n",
    "\t\timages=image_inputs,\n",
    "\t\tvideos=video_inputs,\n",
    "\t\tfps=fps,\n",
    "\t\tpadding=True,\n",
    "\t\treturn_tensors=\"pt\",\n",
    "\t\t\n",
    "\t)\n",
    "\tinputs = inputs.to(\"cuda\")\n",
    "\n",
    "\t# Inference\n",
    "\tgenerated_ids = qwen_vl_model.generate(**inputs, max_new_tokens=128)\n",
    "\tgenerated_ids_trimmed = [\n",
    "\t\tout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "\t]\n",
    "\toutput_text = qwen_vl_processor.batch_decode(\n",
    "\t\tgenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "\t)\n",
    "\tprint(output_text)\n",
    "\treturn output_text\n",
    "\n",
    "previous_qwen_state = \"None because its the first clip\"\n",
    "previous_information = \"None because its the first clip\"\n",
    "current_dino_state = \"None because its the first clip\"\n",
    "\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "for idx in range(0, len(frames_to_process), 9):\n",
    "\timage_paths = []\n",
    "\tfor frame_idx, frame in frames_to_process[idx:idx+9]:\n",
    "\t\tframe_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\t\timage = Image.fromarray(frame_rgb)\n",
    "\t\timage_path = os.path.join(temp_images_dir, f\"frame_{frame_idx}_original.jpg\")\n",
    "\t\timage.save(image_path)\n",
    "\t\timage_paths.append(f'file://{image_path}')\n",
    "\tprint(\"image_paths:\", image_paths)\n",
    "\t# yolo_results = yolo_model(frame)\n",
    "\t# skip_to_qwen = False\n",
    "\t# if len(yolo_results) == 0 or len(yolo_results[0].boxes) == 0:\n",
    "\t# \tprint(f\"Frame {frame_idx}: No detection found. Skipping frame.\")\n",
    "\t# \tskip_to_qwen = True\n",
    "\ttrue_label = frame_to_class[frame_idx]\n",
    "\ttrue_label_name = category_id_to_name.get(true_label, f\"Class_{true_label}\")\n",
    "\tif not True:\n",
    "\t\tboxes = yolo_results[0].boxes.data\t# Each row: [x1, y1, x2, y2, conf, cls]\n",
    "\n",
    "\t\t# Find the box with the highest confidence\n",
    "\t\thighest_conf_idx = -1\n",
    "\t\thighest_conf = 0\n",
    "\n",
    "\t\tfor idx, box in enumerate(boxes):\n",
    "\t\t\tconfidence = box[4].item()\n",
    "\t\t\tif confidence < YOL_THRESHOLD:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t\n",
    "\t\t\tif confidence > highest_conf:\n",
    "\t\t\t\thighest_conf = confidence\n",
    "\t\t\t\thighest_conf_idx = idx\n",
    "\t\t\n",
    "\t\tif highest_conf_idx >= 0:\n",
    "\t\t\tbox = boxes[highest_conf_idx]\n",
    "\t\t\tconfidence = box[4].item()\n",
    "\t\t\tpredicted_label = int(box[5].item())\n",
    "\t\t\tcurrent_dino_state = category_id_to_name.get(predicted_label, f\"Class_{predicted_label}\")\n",
    "\t\t\t\n",
    "\t\t\tx1, y1, x2, y2 = map(int, box[:4].tolist())\n",
    "\t\t\tx1 = max(0, x1)\n",
    "\t\t\ty1 = max(0, y1)\n",
    "\t\t\tx2 = min(image.width, x2)\n",
    "\t\t\ty2 = min(image.height, y2)\n",
    "\t\t\t\n",
    "\t\t\tcropped_image = image.crop((x1, y1, x2, y2))\n",
    "\t\t\tcropped_path = os.path.join(temp_images_dir, f\"frame_{frame_idx}.jpg\")\n",
    "\t\t\tcropped_image.save(cropped_path)\n",
    "\t\t\t\n",
    "\t\t\tprint(f\"Saved cropped image: {cropped_path}\")\n",
    "\t\telse:\n",
    "\t\t\tcurrent_dino_state = \"None, Dino didnt make a prediction\"\n",
    "\t\t\t\n",
    "\tdisplay_images_horizontal([img.replace('file://', '') for img in image_paths])\n",
    "\tres = qwen_vl(image_paths, current_dino_state, previous_qwen_state, previous_information)[0]\n",
    "\tres = json.loads(res)\n",
    "\n",
    "\tprevious_qwen_state = res[\"state\"]\n",
    "\tprevious_information = res[\"action_description\"]\n",
    "\tpredicted_label = previous_qwen_state\n",
    "\tis_correct = labels_match(previous_qwen_state, true_label)\n",
    "\tif is_correct:\n",
    "\t\tcorrect_predictions += 1\n",
    "\ttotal_predictions += 1\n",
    "\n",
    "\tprint(f\"Frame {frame_idx}\")\n",
    "\tprint(f\"\tTrue:\t{true_label}\")\n",
    "\tprint(f\"\tPredicted: {predicted_label}\")\n",
    "\tprint(f\"\tCorrect:\t {'✅ Yes' if is_correct else '❌ No'}\")\n",
    "\tprint(\"-\" * 30)\n",
    "\n",
    "\n",
    "\n",
    "accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "print(\"\\nEvaluation Summary:\")\n",
    "print(f\"\tTotal predictions: {total_predictions}\")\n",
    "print(f\"\tCorrect predictions: {correct_predictions}\")\n",
    "print(f\"\tAccuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
