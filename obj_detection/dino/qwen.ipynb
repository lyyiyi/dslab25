{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore augmentations with these names:\n",
    "- _colorpatch\n",
    "- _sam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# STAGE_DESCRIPTIONS = {\n",
    "# \t \"stage_0\": \"I can see the base plate, therefore the object must be in stage_0\",\n",
    "# \t \"stage_1\": \"I can see the cylinder being on top of the base plate, therefore the object must be in stage_1\",\n",
    "# \t \"stage_2\": \"I can see the big metal piece being on top of the cylinder, therefore the object must be in stage_2\",\n",
    "# \t \"stage_3\": \"I can see the small metal piece being on placed in the middle of the big metal piece, therefore the object must be in stage_3\",\n",
    "# \t \"stage_4\": \"I can see the small metal ring being placed in the center of the small metal piece, therefore the object must be in stage_4\",\n",
    "# \t \"stage_5\": \"I can see the 3 screws being screwed on the metal plate, therefore the object must be in stage_5\",\n",
    "# \t \"stage_6\": \"I can see the daker metal plate being placed on top of the object, therefore the object must be in stage_6\",\n",
    "# \t \"stage_7\": \"I can see 5 screws being screwed on the metal plate, therefore the object must be in stage_7\",\n",
    "# }\n",
    "\n",
    "STAGE_DESCRIPTIONS = {\n",
    "\t\"stage_0\": \"I can see the base plate, which is the main piece of stage_0\",\n",
    "\t\"stage_1\": \"I can see the cylinder the main piece of stage_1\",\n",
    "\t\"stage_2\": \"I can see the big metal piecethe main piece of stage_2\",\n",
    "\t\"stage_3\": \"I can see the smaller metal thinner piece the main piece of stage_3\",\n",
    "\t\"stage_4\": \"I can see the small metal ring the main piece of stage_5\",\n",
    "\t\"stage_5\": \"I can see 3 screws\",\n",
    "\t\"stage_6\": \"I can see the darker metal plate the main piece of stage_6\",\n",
    "\t\"stage_7\": \"I can see 5 screws\",\n",
    "}\n",
    "\n",
    "\n",
    "def process_stage(stage: str, src_root, dst_root) -> None:\n",
    "\t\"\"\"Copy images for one stage and write annotation files.\n",
    "\n",
    "\t`src_root` and `dst_root` can be Path objects **or** plain strings.\"\"\"\n",
    "\tsrc_root = Path(src_root)\n",
    "\tdst_root = Path(dst_root)\n",
    "\n",
    "\tsrc_dir = src_root / \"images\" / stage\n",
    "\tif not src_dir.is_dir():\n",
    "\t\tprint(f\"Warning: {src_dir} is missing, skipping.\")\n",
    "\t\treturn\n",
    "\n",
    "\tdst_img_dir = dst_root / \"images\" / stage\n",
    "\tdst_ann_dir = dst_root / \"anno\" / stage\n",
    "\tdst_img_dir.mkdir(parents=True, exist_ok=True)\n",
    "\tdst_ann_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\tdesc_text = STAGE_DESCRIPTIONS[stage]\n",
    "\n",
    "\tfor img_path in src_dir.iterdir():\n",
    "\t\tif img_path.is_dir():\n",
    "\t\t\tcontinue\n",
    "\t\tname = img_path.name\n",
    "\t\tif \"_colorpatch\" in name or \"_sam\" in name:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tshutil.copy2(img_path, dst_img_dir / name.replace('.jpg', '_solo.jpg'))\n",
    "\t\t(dst_ann_dir / f\"{img_path.stem}_solo.txt\").write_text(desc_text, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "# Example manual call (now works with strings):\n",
    "for stage in STAGE_DESCRIPTIONS:\n",
    "\tprocess_stage(\n",
    "\t\tstage,\n",
    "\t\t\"/Users/georgye/Documents/repos/ethz/dslab25/assets/vacuum_pump/rendered_single\",\n",
    "\t\t\"/Users/georgye/Documents/repos/ethz/dslab25/training/qwen\",\n",
    "\t)\n",
    "\tprocess_stage(\n",
    "\t\tstage,\n",
    "\t\t\"/Users/georgye/Documents/repos/ethz/dslab25/assets/vacuum_pump/rendered\",\n",
    "\t\t\"/Users/georgye/Documents/repos/ethz/dslab25/training/qwen\",\n",
    "\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting whole folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  Done.\n",
      "• Train images: 2675 ➜ /Users/georgye/Documents/repos/ethz/dslab25/training/qwen/data/train\n",
      "• Val   images: 669 ➜ /Users/georgye/Documents/repos/ethz/dslab25/training/qwen/data/val\n",
      "• JSONL files:  /Users/georgye/Documents/repos/ethz/dslab25/training/qwen/data/train/annotations.jsonl, /Users/georgye/Documents/repos/ethz/dslab25/training/qwen/data/val/annotations.jsonl\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "prepare_qwen25vl_data.py\n",
    "------------------------\n",
    "Build the folder hierarchy and JSONL annotation files required to fine-tune\n",
    "Qwen-2.5-VL on your assembly-stage images.\n",
    "\n",
    "Images:  {images_root}/stage_k/XXXX.jpg\n",
    "Labels:  {labels_root}/stage_k/XXXX.txt   (same relative path & basename)\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "SYSTEM_MESSAGE = (\n",
    "\t\"You job is it tell me what you see in the image, and if possible what stage \"\n",
    "\t\"the object is currently in.\\nHere are the possible states:\\n\"\n",
    "\t\"\\t'state_0': 'First part of the object: Base block metal piece',\\n\"\n",
    "\t\"\\t'state_1': 'Second part of the object: Cylinder metal piece which gets stick \"\n",
    "\t\"on the base block stage_0',\\n\"\n",
    "\t\"\\t'state_2': 'Third part of the object: A Big metal piece which gets stick on \"\n",
    "\t\"the cylinder piece of stage_1',\\n\"\n",
    "\t\"\\t'state_3': 'Fourth part of the object: A smaller thin metal piece which gets \"\n",
    "\t\"put onto the center of the big metal piece of stage_2',\\n\"\n",
    "\t\"\\t'state_4': 'Fifth part of the object: A tiny metal ring which gets placed \"\n",
    "\t\"onto the center of the thing metal piece of stage_3',\\n\"\n",
    "\t\"\\t'state_5': 'Sixth part of the object: 3 screws now get screwed onto the piece',\\n\"\n",
    "\t\"\\t'state_6': 'Seventh part of the object: A darker metal plate now gets placed \"\n",
    "\t\"on top of the piece',\\n\"\n",
    "\t\"\\t'state_7': 'Eighth part of the object: 5 screws now get screwed onto the piece'\"\n",
    ")\n",
    "\n",
    "USER_PREFIX = (\n",
    "\t\"Describe the object and, if you can, tell me which stage (state_0 … state_7) \"\n",
    "\t\"it is currently in.\"\n",
    ")\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "\n",
    "def collect_pairs(img_root: Path, lbl_root: Path) -> List[Tuple[Path, str]]:\n",
    "\t\"\"\"Return list of (image_path, label_text).\"\"\"\n",
    "\timg_root = Path(img_root)\n",
    "\tlbl_root = Path(lbl_root)\n",
    "\n",
    "\tpairs: List[Tuple[Path, str]] = []\n",
    "\tfor img_path in img_root.rglob(\"*.jpg\"):\n",
    "\t\trel = img_path.relative_to(img_root)\n",
    "\t\tlbl_path = lbl_root / rel.with_suffix(\".txt\")\n",
    "\t\tif not lbl_path.exists():\n",
    "\t\t\traise FileNotFoundError(f\"Missing label file for image: {img_path}\")\n",
    "\t\ttext = lbl_path.read_text(encoding=\"utf-8\").strip()\n",
    "\t\tpairs.append((img_path, text))\n",
    "\n",
    "\tif not pairs:\n",
    "\t\traise RuntimeError(\"No image/label pairs found - check your paths.\")\n",
    "\treturn pairs\n",
    "\n",
    "\n",
    "def split_pairs(\n",
    "\tpairs: List[Tuple[Path, str]], train_split: float, seed: int\n",
    ") -> Tuple[List[Tuple[Path, str]], List[Tuple[Path, str]]]:\n",
    "\trandom.Random(seed).shuffle(pairs)\n",
    "\tcut = int(len(pairs) * train_split)\n",
    "\treturn pairs[:cut], pairs[cut:]\n",
    "\n",
    "\n",
    "def write_jsonl(\n",
    "\tpairs: List[Tuple[Path, str]], out_dir: Path, img_root: Path\n",
    ") -> None:\n",
    "\t\"\"\"\n",
    "\tCopy images into out_dir and create annotations.jsonl\n",
    "\t(images are kept at top level of out_dir; stage_*/ sub-folders are replicated).\n",
    "\t\"\"\"\n",
    "\tout_dir = Path(out_dir)\n",
    "\timg_root = Path(img_root)\n",
    "\tanno_path = out_dir / \"annotations.jsonl\"\n",
    "\tout_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\twith anno_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "\t\tfor img_path, label in pairs:\n",
    "\t\t\trel_img = img_path.relative_to(img_root)\n",
    "\t\t\tdest_img = out_dir / rel_img\n",
    "\t\t\tdest_img.parent.mkdir(parents=True, exist_ok=True)\n",
    "\t\t\tshutil.copy2(img_path, dest_img)\n",
    "\n",
    "\t\t\trecord = {\n",
    "\t\t\t\t\"image\": str(rel_img).replace(\"\\\\\", \"/\"),  # JSONL wants forward slashes\n",
    "\t\t\t\t\"prefix\": USER_PREFIX,\n",
    "\t\t\t\t\"suffix\": label,\n",
    "\t\t\t}\n",
    "\t\t\tf.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "# ─── USER-SPECIFIC PATHS ────────────────────────────────────────────────────────\n",
    "IMG_ROOT  = Path(\"/Users/georgye/Documents/repos/ethz/dslab25/training/qwen/images/augmented\")\n",
    "LBL_ROOT  = Path(\"/Users/georgye/Documents/repos/ethz/dslab25/training/qwen/annotation/augmented\")\n",
    "DATA_ROOT = Path(\"/Users/georgye/Documents/repos/ethz/dslab25/training/qwen/data\")\n",
    "TRAIN_SPLIT = 0.95\n",
    "SEED = 32\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# 1. Pair up images and labels\n",
    "pairs = collect_pairs(IMG_ROOT, LBL_ROOT)\n",
    "\n",
    "# 2. Train / val split\n",
    "train_pairs, val_pairs = split_pairs(pairs, TRAIN_SPLIT, SEED)\n",
    "\n",
    "# 3. Write datasets\n",
    "train_dir = DATA_ROOT / \"train\"\n",
    "val_dir   = DATA_ROOT / \"val\"\n",
    "\n",
    "for dirpath in (train_dir, val_dir):\n",
    "\t\tif dirpath.exists():\n",
    "\t\t\t\tshutil.rmtree(dirpath)  # start clean\n",
    "\n",
    "write_jsonl(train_pairs, train_dir, IMG_ROOT)\n",
    "write_jsonl(val_pairs,  val_dir,  IMG_ROOT)\n",
    "\n",
    "# 4. Save the system prompt\n",
    "(DATA_ROOT / \"system_message.txt\").write_text(SYSTEM_MESSAGE, encoding=\"utf-8\")\n",
    "\n",
    "# 5. Report\n",
    "print(\n",
    "\t\tf\"✅  Done.\\n\"\n",
    "\t\tf\"• Train images: {len(train_pairs)} ➜ {train_dir}\\n\"\n",
    "\t\tf\"• Val   images: {len(val_pairs)} ➜ {val_dir}\\n\"\n",
    "\t\tf\"• JSONL files:  {train_dir/'annotations.jsonl'}, {val_dir/'annotations.jsonl'}\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "finetune_qwen25vl7b.py\n",
    "──────────────────────\n",
    "Fine-tune Qwen-2.5-VL-7B-Instruct (Hugging Face) on a local JSONL+image dataset.\n",
    "\n",
    "Requires:\n",
    "  pip install -q \"git+https://github.com/huggingface/transformers\" \\\n",
    "\t\t\t\t accelerate peft bitsandbytes qwen-vl-utils[decord]==0.0.8 \\\n",
    "\t\t\t\t lightning nltk\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Any, List, Tuple\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "from nltk import edit_distance\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (\n",
    "\tBitsAndBytesConfig,\n",
    "\tQwen2_5_VLForConditionalGeneration,\n",
    "\tQwen2_5_VLProcessor,\n",
    ")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "SYSTEM_MESSAGE = Path(\n",
    "\t__file__\n",
    ").with_name(\"system_message.txt\").read_text(encoding=\"utf-8\")\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "\n",
    "def format_chat(image_dir: Path, entry: dict) -> List[dict]:\n",
    "\t\"\"\"Return a 3-turn conversation in the format Qwen expects.\"\"\"\n",
    "\treturn [\n",
    "\t\t{\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": SYSTEM_MESSAGE}]},\n",
    "\t\t{\n",
    "\t\t\t\"role\": \"user\",\n",
    "\t\t\t\"content\": [\n",
    "\t\t\t\t{\"type\": \"image\", \"image\": str(image_dir / entry[\"image\"])},\n",
    "\t\t\t\t{\"type\": \"text\", \"text\": entry[\"prefix\"]},\n",
    "\t\t\t],\n",
    "\t\t},\n",
    "\t\t{\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": entry[\"suffix\"]}]},\n",
    "\t]\n",
    "\n",
    "\n",
    "class JSONLDataset(Dataset):\n",
    "\tdef __init__(self, jsonl_path: Path, image_dir: Path):\n",
    "\t\tself.image_dir = image_dir\n",
    "\t\tself.entries = [json.loads(l) for l in jsonl_path.read_text().splitlines()]\n",
    "\n",
    "\tdef __len__(self):  # noqa: D401\n",
    "\t\treturn len(self.entries)\n",
    "\n",
    "\tdef __getitem__(self, idx: int) -> Tuple[Any, dict, List[dict]]:  # noqa: D401\n",
    "\t\tentry = self.entries[idx]\n",
    "\t\treturn None, entry, format_chat(self.image_dir, entry)\n",
    "\n",
    "\n",
    "# ─── Collate fns ────────────────────────────────────────────────────────────────\n",
    "def make_collate(processor):\n",
    "\tdef train_collate(batch):\n",
    "\t\t_, _, examples = zip(*batch)\n",
    "\t\ttexts = [processor.apply_chat_template(e, tokenize=False) for e in examples]\n",
    "\t\timgs = [process_vision_info(e)[0] for e in examples]\n",
    "\t\tmodel_in = processor(text=texts, images=imgs, return_tensors=\"pt\", padding=True)\n",
    "\t\tlabels = model_in[\"input_ids\"].clone()\n",
    "\t\tlabels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "\t\tfor tkn in (151652, 151653, 151655):\n",
    "\t\t\tlabels[labels == tkn] = -100\n",
    "\t\treturn (\n",
    "\t\t\tmodel_in[\"input_ids\"],\n",
    "\t\t\tmodel_in[\"attention_mask\"],\n",
    "\t\t\tmodel_in[\"pixel_values\"],\n",
    "\t\t\tmodel_in[\"image_grid_thw\"],\n",
    "\t\t\tlabels,\n",
    "\t\t)\n",
    "\n",
    "\tdef eval_collate(batch):\n",
    "\t\t_, data, examples = zip(*batch)\n",
    "\t\tsuffixes = [d[\"suffix\"] for d in data]\n",
    "\t\tprompts = [processor.apply_chat_template(e[:2], tokenize=False) for e in examples]\n",
    "\t\timgs = [process_vision_info(e[:2])[0] for e in examples]\n",
    "\t\tmodel_in = processor(text=prompts, images=imgs, return_tensors=\"pt\", padding=True)\n",
    "\t\treturn (\n",
    "\t\t\tmodel_in[\"input_ids\"],\n",
    "\t\t\tmodel_in[\"attention_mask\"],\n",
    "\t\t\tmodel_in[\"pixel_values\"],\n",
    "\t\t\tmodel_in[\"image_grid_thw\"],\n",
    "\t\t\tsuffixes,\n",
    "\t\t)\n",
    "\n",
    "\treturn train_collate, eval_collate\n",
    "\n",
    "\n",
    "# ─── Lightning module ───────────────────────────────────────────────────────────\n",
    "class QwenTrainer(L.LightningModule):\n",
    "\tdef __init__(self, cfg, model, processor, train_set, val_set):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.save_hyperparameters()\n",
    "\t\tself.cfg, self.model, self.processor = cfg, model, processor\n",
    "\t\tself.train_set, self.val_set = train_set, val_set\n",
    "\t\tself.train_collate, self.eval_collate = make_collate(processor)\n",
    "\n",
    "\t# ╭─ training ╮\n",
    "\tdef training_step(self, batch, _):\n",
    "\t\tids, msk, pix, thw, lbl = batch\n",
    "\t\tloss = self.model(\n",
    "\t\t\tinput_ids=ids,\n",
    "\t\t\tattention_mask=msk,\n",
    "\t\t\tpixel_values=pix,\n",
    "\t\t\timage_grid_thw=thw,\n",
    "\t\t\tlabels=lbl,\n",
    "\t\t).loss\n",
    "\t\tself.log(\"train_loss\", loss, prog_bar=True)\n",
    "\t\treturn loss\n",
    "\n",
    "\t# ╭─ validation ╮\n",
    "\tdef validation_step(self, batch, _):\n",
    "\t\tids, msk, pix, thw, refs = batch\n",
    "\t\tgen_ids = self.model.generate(\n",
    "\t\t\tinput_ids=ids,\n",
    "\t\t\tattention_mask=msk,\n",
    "\t\t\tpixel_values=pix,\n",
    "\t\t\timage_grid_thw=thw,\n",
    "\t\t\tmax_new_tokens=256,\n",
    "\t\t)\n",
    "\t\touts = self.processor.batch_decode(\n",
    "\t\t\t[o[len(i) :] for i, o in zip(ids, gen_ids)],\n",
    "\t\t\tskip_special_tokens=True,\n",
    "\t\t)\n",
    "\t\tscore = sum(edit_distance(o, r) / max(len(o), len(r)) for o, r in zip(outs, refs))\n",
    "\t\tself.log(\"val_edit_dist\", score / len(refs), prog_bar=True)\n",
    "\n",
    "\t# ╭─ loaders ╮\n",
    "\tdef train_dataloader(self):\n",
    "\t\treturn DataLoader(\n",
    "\t\t\tself.train_set,\n",
    "\t\t\tbatch_size=self.cfg[\"batch_size\"],\n",
    "\t\t\tshuffle=True,\n",
    "\t\t\tnum_workers=4,\n",
    "\t\t\tcollate_fn=self.train_collate,\n",
    "\t\t)\n",
    "\n",
    "\tdef val_dataloader(self):\n",
    "\t\treturn DataLoader(\n",
    "\t\t\tself.val_set,\n",
    "\t\t\tbatch_size=1,\n",
    "\t\t\tshuffle=False,\n",
    "\t\t\tnum_workers=2,\n",
    "\t\t\tcollate_fn=self.eval_collate,\n",
    "\t\t)\n",
    "\n",
    "\t# ╭─ optim ╮\n",
    "\tdef configure_optimizers(self):\n",
    "\t\treturn AdamW(self.model.parameters(), lr=self.cfg[\"lr\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ─── Entry-point ────────────────────────────────────────────────────────────────\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"--data_root\", required=True, type=Path)\n",
    "ap.add_argument(\"--epochs\", default=10, type=int)\n",
    "ap.add_argument(\"--save_dir\", default=\"qwen_2_5_vl_7b_ft\", type=Path)\n",
    "args = ap.parse_args()\n",
    "\n",
    "train_jsonl = args.data_root / \"train\" / \"annotations.jsonl\"\n",
    "val_jsonl = args.data_root / \"val\" / \"annotations.jsonl\"\n",
    "\n",
    "train_set = JSONLDataset(train_jsonl, args.data_root / \"train\")\n",
    "val_set = JSONLDataset(val_jsonl, args.data_root / \"val\")\n",
    "\n",
    "# ─── Model + processor ────────────────────────────────────────────────────\n",
    "MODEL_ID = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "lora_cfg = LoraConfig(\n",
    "\t\tr=8, lora_alpha=16, lora_dropout=0.05, bias=\"none\", target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "\t\tload_in_4bit=True,\n",
    "\t\tbnb_4bit_use_double_quant=True,\n",
    "\t\tbnb_4bit_quant_type=\"nf4\",\n",
    "\t\tbnb_4bit_compute_type=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "\t\tMODEL_ID, device_map=\"auto\", quantization_config=bnb_cfg, torch_dtype=torch.bfloat16\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "processor = Qwen2_5_VLProcessor.from_pretrained(MODEL_ID, min_pixels=256 * 28 * 28, max_pixels=1280 * 28 * 28)\n",
    "\n",
    "cfg = dict(batch_size=1, lr=2e-4)\n",
    "lit = QwenTrainer(cfg, model, processor, train_set, val_set)\n",
    "\n",
    "# ─── Checkpoint callback ──────────────────────────────────────────────────\n",
    "class SaveBoth(L.Callback):\n",
    "\t\tdef __init__(self, out: Path):  # noqa: D401\n",
    "\t\t\t\tself.out = out\n",
    "\n",
    "\t\tdef on_train_epoch_end(self, trainer, pl_module):\n",
    "\t\t\t\tpath = self.out / f\"epoch_{trainer.current_epoch}\"\n",
    "\t\t\t\tpath.mkdir(parents=True, exist_ok=True)\n",
    "\t\t\t\tpl_module.processor.save_pretrained(path)\n",
    "\t\t\t\tpl_module.model.save_pretrained(path)\n",
    "\t\t\t\tprint(f\"[ckpt] {path}\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "\t\taccelerator=\"gpu\",\n",
    "\t\tdevices=1,\n",
    "\t\tmax_epochs=args.epochs,\n",
    "\t\taccumulate_grad_batches=8,\n",
    "\t\tgradient_clip_val=1.0,\n",
    "\t\tlog_every_n_steps=10,\n",
    "\t\tcallbacks=[SaveBoth(args.save_dir)],\n",
    "\t\tprecision=\"bf16-mixed\",\n",
    ")\n",
    "\n",
    "trainer.fit(lit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dslab_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
